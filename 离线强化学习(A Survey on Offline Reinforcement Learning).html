<h1 style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif;">离线强化学习(A Survey on Offline Reinforcement Learning)</span></h1>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">作者：凯鲁嘎吉 - 博客园&nbsp;<a href="http://www.cnblogs.com/kailugaji/" rel="noopener" target="_blank">http://www.cnblogs.com/kailugaji/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; 通过阅读《<a href="https://arxiv.org/abs/2203.01387" target="_blank">A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems</a>》与《<a href="https://arxiv.org/abs/2005.01643" target="_blank">Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems</a>》这两篇关于离线强化学习的综述论文，初步认识离线强化学习，了解离线强化学习的概念、挑战、相关方法(仅粗略介绍，未详细展开)及未来可能的研究方向。更多强化学习内容，请看：<a href="https://www.cnblogs.com/kailugaji/category/2038931.html" rel="noopener" target="_blank">随笔分类 - Reinforcement Learning</a>。</span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">1. Introduction</span></h2>
<h3><span style="font-family: 'comic sans ms', sans-serif;">1.1 Supervised Machine Learning, RL, and Off-policy RL</span></h3>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170436087-246754121.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h3><span style="font-family: 'comic sans ms', sans-serif;">1.2 The Power of Offline RL</span></h3>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170452618-975650673.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h3><span style="font-family: 'comic sans ms', sans-serif;">1.3 On-policy vs. Off-policy</span></h3>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170503676-1003296447.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h3><span style="font-family: 'comic sans ms', sans-serif;">1.4 On-policy, Off-policy, and Offline (Batch) RL</span></h3>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170517979-507477974.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170545831-1817054716.gif" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h3><span style="font-family: 'comic sans ms', sans-serif;">1.5 Imitation Learning, RL, and Offline RL</span></h3>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170601808-2112933160.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">2. Challenges</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170615047-908160557.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170623198-1442539351.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">3. Taxonomy</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170648444-1979970688.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170712587-1542614990.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170721827-2076900883.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170734185-1856067721.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p>&nbsp;</p>
<p style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Illustration of the general structure of an offline RL algorithm</span></p>
<h3><span style="font-family: 'comic sans ms', sans-serif;">3.1 Policy Constraints</span></h3>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170753920-957149020.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h3><span style="font-family: 'comic sans ms', sans-serif;">3.2 Importance Sampling</span></h3>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170808272-584843991.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170816886-706246194.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170825533-416086892.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170835469-773929670.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170847814-2061531637.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170854875-510594460.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170902772-1581250945.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h3><span style="font-family: 'comic sans ms', sans-serif;">3.3 Regularization</span></h3>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170914366-1369787187.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170924227-795640287.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h3><span style="font-family: 'comic sans ms', sans-serif;">3.4 Uncertainty Estimation</span></h3>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170936648-1399129499.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h3><span style="font-family: 'comic sans ms', sans-serif;">3.5 Model-based Methods</span></h3>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170948114-688445888.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h3><span style="font-family: 'comic sans ms', sans-serif;">3.6 One-step Methods</span></h3>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322170958227-685552197.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h3><span style="font-family: 'comic sans ms', sans-serif;">3.7 Imitation Learning</span></h3>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322171010603-1623415401.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322171019968-1375112886.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; 模仿学习资料：</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; &nbsp;许天，李子牛，俞扬，模仿学习简洁教程，2021. <a href="http://www.lamda.nju.edu.cn/xut/Imitation_Learning.pdf" target="_blank">http://www.lamda.nju.edu.cn/xut/Imitation_Learning.pdf</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp;&nbsp;【RLChina 2021】第10课 强化学习前沿（二）俞扬：<a href="https://www.bilibili.com/video/BV1qM4y1L7w9?spm_id_from=333.999.0.0" target="_blank">https://www.bilibili.com/video/BV1qM4y1L7w9?spm_id_from=333.999.0.0</a></span></p>
<h3><span style="font-family: 'comic sans ms', sans-serif;">3.8 Trajectory Optimization</span></h3>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322171031220-1848689045.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">4. Open Problems</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322171042578-82368058.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220322171050167-474946902.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">5. 参考文献</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[1] Rafael Figueiredo Prudencio, Marcos R. O. A. Maximo and Esther Luna Colombini. &ldquo;<a href="https://arxiv.org/abs/2203.01387" target="_blank">A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems</a>&rdquo;(2022).</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[2] Sergey Levine, Aviral Kumar, George Tucker and Justin Fu. &ldquo;<a href="https://arxiv.org/abs/2005.01643" target="_blank">Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems</a>&rdquo;(2020).</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[3] CS 285 Deep Reinforcement Learning <a href="https://rail.eecs.berkeley.edu/deeprlcourse/" target="_blank">https://rail.eecs.berkeley.edu/deeprlcourse/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[4] CS330 Fall 2021 Deep Multi-Task and Meta Learning <a href="https://cs330.stanford.edu/" target="_blank">https://cs330.stanford.edu/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[5] Offline (Batch) Reinforcement Learning: A Review of Literature and Applications <a href="https://danieltakeshi.github.io/2020/06/28/offline-rl/" target="_blank">https://danieltakeshi.github.io/2020/06/28/offline-rl/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[6] RL-Paper-notes <a href="https://github.com/2019ChenGong/RL-Paper-notes" target="_blank">https://github.com/2019ChenGong/RL-Paper-notes</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[7] An Optimistic Perspective on Offline Reinforcement Learning <a href="https://offline-rl.github.io/" target="_blank">https://offline-rl.github.io/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[8] 离线强化学习基准：<a href="https://github.com/rail-berkeley/d4rl" target="_blank">https://github.com/rail-berkeley/d4rl</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[9]&nbsp;【RLChina 2021】第9课 强化学习前沿（一） 卢宗青：<a href="https://www.bilibili.com/video/BV1cQ4y1m7Nn?spm_id_from=333.999.0.0" target="_blank">https://www.bilibili.com/video/BV1cQ4y1m7Nn?spm_id_from=333.999.0.0</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[10]&nbsp;Offline Reinforcement Learning Resources, <a href="https://offlinerl.ai/" target="_blank">https://offlinerl.ai/</a></span></p>