<h1 style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif;">重要性采样(Importance Sampling)&mdash;&mdash;TRPO与PPO的补充</span></h1>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">作者：凯鲁嘎吉 - 博客园&nbsp;<a href="http://www.cnblogs.com/kailugaji/" target="_blank">http://www.cnblogs.com/kailugaji/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; 上两篇博客已经介绍了<a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/kailugaji/p/15388913.html">信赖域策略优化(Trust Region Policy Optimization, TRPO)</a>与<a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/kailugaji/p/15396437.html">近端策略优化算法(Proximal Policy Optimization Algorithms, PPO)</a>，他们用到一个重要的技巧就是：重要性采样。但是都需要限制新旧策略使两者差异不能太大，TRPO通过添加新旧策略的KL约束项，而PPO是限制两者比率的变化范围，这究竟是为什么呢？不加这个约束会怎样？下面通过对重要性采样进行分析，来解答这个问题。更多强化学习内容，请看：<a href="https://www.cnblogs.com/kailugaji/category/2038931.html" target="_blank">随笔分类 - Reinforcement Learning</a>。</span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">1.&nbsp;采样法(Sampling Method)/蒙特卡罗方法(Monte Carlo Method)</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211013143337460-2013568540.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2 id="1634095709971"><span style="font-family: 'comic sans ms', sans-serif;">2.&nbsp;重要性采样(Importance Sampling)</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211013112858985-1134598392.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211013112906490-1926128536.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2 id="1634095745554"><span style="font-family: 'comic sans ms', sans-serif;">3.&nbsp;重新思考TRPO与PPO</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211013112929398-1203607711.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2 id="1634095768473"><span style="font-family: 'comic sans ms', sans-serif;">4. 参考文献</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[1] 茆诗松, 程依明, 濮晓龙. 概率论与数理统计教程. 高等教育出版社, 2011.</span><br /><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[2] 邱锡鹏，神经网络与深度学习，机械工业出版社，<a href="https://nndl.github.io/" target="_blank">https://nndl.github.io/</a>, 2020.</span><br /><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[3] 李宏毅, 强化学习课程, <a href="https://www.bilibili.com/video/BV1UE411G78S?spm_id_from=333.999.0.0" target="_blank">https://www.bilibili.com/video/BV1UE411G78S?spm_id_from=333.999.0.0</a>, 2020.</span></p>