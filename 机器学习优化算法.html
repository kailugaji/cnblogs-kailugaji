<h1 style="text-align: center;">机器学习优化算法</h1>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">作者：凯鲁嘎吉 - 博客园&nbsp;<a href="http://www.cnblogs.com/kailugaji/" target="_blank">http://www.cnblogs.com/kailugaji/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211014093306054-1105977361.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">图来源：<a href="http://www.cs.virginia.edu/~hw5x/Course/RL2020-Fall/_site/static_files/ppt/policy-grad.pptx" target="_blank">http://www.cs.virginia.edu/~hw5x/Course/RL2020-Fall/_site/static_files/ppt/policy-grad.pptx</a></span></p>
<h2>1. 梯度下降法</h2>
<p><img src="https://img2018.cnblogs.com/i-beta/1027447/201912/1027447-20191212165458101-205451579.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><img src="https://img2018.cnblogs.com/i-beta/1027447/201912/1027447-20191212165341247-968876384.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><img src="https://img2018.cnblogs.com/i-beta/1027447/201912/1027447-20191212165401417-1301001313.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><img src="https://img2018.cnblogs.com/i-beta/1027447/201912/1027447-20191212165421221-286358559.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<h2>2. 随机梯度下降法</h2>
<p><img src="https://img2018.cnblogs.com/i-beta/1027447/201912/1027447-20191212165536664-1364823356.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><img src="https://img2018.cnblogs.com/i-beta/1027447/201912/1027447-20191212165556545-1632380913.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><img src="https://img2018.cnblogs.com/i-beta/1027447/201912/1027447-20191212165629344-2032835132.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<h2>3. 小批量梯度下降法</h2>
<p><img src="https://img2018.cnblogs.com/i-beta/1027447/201912/1027447-20191212165653631-938738696.png" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<h2>4. 参考文献</h2>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[1] 邱锡鹏,&nbsp;<a href="https://nndl.github.io/" target="_blank">神经网络与深度学习</a>[M]. 2019.</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[2]&nbsp;<a id="cb_post_title_url" class="postTitle2" href="https://www.cnblogs.com/maybe2030/p/5089753.html">[Machine Learning] 梯度下降法的三种形式BGD、SGD以及MBGD</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;"><a id="cb_post_title_url" class="postTitle2" href="https://www.cnblogs.com/maybe2030/p/5089753.html"></a>[3]&nbsp;Ruder, Sebastian . "<a href="https://arxiv.org/pdf/1609.04747.pdf" target="_blank">An overview of gradient descent optimization algorithms</a>." (2016).</span></p>