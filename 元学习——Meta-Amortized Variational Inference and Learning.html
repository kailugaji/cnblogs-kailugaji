<h1 style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif;">元学习&mdash;&mdash;Meta-Amortized Variational Inference and Learning</span></h1>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">作者：凯鲁嘎吉 - 博客园&nbsp;<a href="http://www.cnblogs.com/kailugaji/" target="_blank">http://www.cnblogs.com/kailugaji/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; 这篇博客是论文&ldquo;<a href="https://ojs.aaai.org//index.php/AAAI/article/view/6111" target="_blank">Meta-Amortized Variational Inference and Learning</a>&rdquo;的阅读笔记。博客中前半部分内容与变分自编码器(VAE)的推导极为类似，所涉及的公式推导如果有不明白的地方，可以提前阅读这篇博客：<a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/kailugaji/p/12463966.html">变分推断与变分自编码器</a>。主要涉及到了概率论的相关知识。这篇博客介绍了精确推理(Exact Inference)，近似变分推理(Approximate Variational Inference)，摊销变分推理(Amortized Variational Inference)，摊销变分自编码器(Amortized Variational Autoencoders)，元摊销变分推理(Meta-Amortized Variational Inference)，以及论文中提出的元变分自编码器(MetaVAE)。主要阅读了论文原理部分，实验部分没有去关注。</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; 尽管最近在概率建模及其应用方面取得了成功，但使用传统推理技术训练的生成模型很难适应新的分布，即使目标分布可能与训练中已见过的分布密切相关。这篇文章提出了一个双重摊销变分推理模型，以解决这一挑战。通过不仅在一组查询输入中共享计算，而且在一组不同但相关的概率模型中共享计算，所提算法学习到了可迁移的潜在表示，这些表示可在多个相关的分布中进行拓展。特别地，给定一组在图像上的分布，所提算法找到了学习表示，以迁移到不同的数据变换。在MNIST(10-50%)和NORB(10-35%)上，通过引入MetaVAE验证了该方法的有效性，并表明该方法在下游图像分类任务中显著优于基准结果。</span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">1. Exact and Approximate Inference (精确与近似推理)</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211105155422220-948834857.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">2.&nbsp;Amortized Variational Inference (摊销变分推理)</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211105155446990-1214895566.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">3. 近似变分推理 vs 摊销变分推理</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211105155513287-819504417.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">4.&nbsp;Amortized Variational Autoencoders (摊销变分自编码器)</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211105155531554-26136564.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">这部分内容推导与<a href="https://www.cnblogs.com/kailugaji/p/12463966.html#_lab2_0_2" target="_blank">直面联合分布</a>很类似，可以参看：<a href="https://www.cnblogs.com/kailugaji/p/12463966.html#_lab2_0_2" target="_blank">https://www.cnblogs.com/kailugaji/p/12463966.html#_lab2_0_2</a></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">5.&nbsp;Meta-Amortized Variational Inference (元摊销变分推理)</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211105155551509-356224777.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211105155559270-1303684502.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211105155606853-1983584181.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211105155614233-961731837.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211105155621017-1499627412.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">6. Related Works (VAE, Neural Statistician, VHE, and MetaVAE)</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211105155637512-281303298.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">7.&nbsp;参考文献</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[1] Mike Wu, Kristy Choi, Noah Goodman, and Stefano Ermon. Meta-Amortized Variational Inference and Learning. AAAI, 2020.</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Paper: <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6111">https://ojs.aaai.org//index.php/AAAI/article/view/6111</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Code: <a href="https://github.com/mhw32/meta-inference-public">https://github.com/mhw32/meta-inference-public</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[2] CS236, Meta-Amortized Variational Inference and Learning. <a href="https://deepgenerativemodels.github.io/assets/slides/meta_amortized.pdf">https://deepgenerativemodels.github.io/assets/slides/meta_amortized.pdf</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[3] Variational Inference: Foundations and Modern Methods, P100 Amortizing Inference, 2016, <a href="https://media.nips.cc/Conferences/2016/Slides/6199-Slides.pdf">https://media.nips.cc/Conferences/2016/Slides/6199-Slides.pdf</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[4] 2021 Pyro Fundamentals, Amortized Inference, and Variational Autoencoders, <a href="https://robsalomone.com/wp-content/uploads/2021/07/L4_VAE.pdf">https://robsalomone.com/wp-content/uploads/2021/07/L4_VAE.pdf</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[5] Rui Shu, Hung H. Bui, Shengjia Zhao, Mykel J. Kochenderfer, Stefano Ermon. Amortized Inference Regularization. NeurIPS 2018, <a href="https://papers.nips.cc/paper/2018/hash/1819932ff5cf474f4f19e7c7024640c2-Abstract.html">https://papers.nips.cc/paper/2018/hash/1819932ff5cf474f4f19e7c7024640c2-Abstract.html</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[6]&nbsp;Amortized Optimization - Rui Shu <a href="http://ruishu.io/2017/11/07/amortized-optimization/" target="_blank">http://ruishu.io/2017/11/07/amortized-optimization/</a></span></p>