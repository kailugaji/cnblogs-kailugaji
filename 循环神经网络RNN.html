<h1 style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif;">循环神经网络RNN</span></h1>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">作者：凯鲁嘎吉 - 博客园&nbsp;<a href="http://www.cnblogs.com/kailugaji/" target="_blank">http://www.cnblogs.com/kailugaji/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; 在前馈神经网络中，信息的传递是单向的，这种限制虽然使得网络变得更容易学习，但在一定程度上也减弱了神经网络模型的能力，在生物神经网络中，神经元之间的连接关系要复杂得多，前馈神经网络可以看作一个复杂的函数，每次输入都是独立的，即网络的输出只依赖于当前的输入，但是在很多现实任务中网络的输出不仅和当前时刻的输入相关，也和其过去一段时间的输出相关。比如一个有限状态自动机，其下一个时刻的状态(输出)不仅仅和当前输入相关，也和当前状态(上一个时刻的输出)相关，此外，前馈网络难以处理时序数据，比如视频、语音、文本等，时序数据的长度一般是不固定的，而前馈神经网络要求输入和输出的维数都是固定的，不能任意改变因此，当处理这一类和时序数据相关的问题时，就需要一种能力更强的模型。</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; 循环神经网络(Recurrent Neural Network，RNN)是一类具有短期记忆能力的神经网络，在循环神经网络中，神经元不但可以接受其他神经元的信息，也可以接受自身的信息，形成具有环路的网络结构，和前馈神经网络相比，循环神经网络更加符合生物神经网络的结构，循环神经网络已经被广泛应用在语音识别、语言模型以及自然语言生成等任务上循环神经网络的参数学习可以通过随时间反向传播算法来学习，随时间反向传播算法即按照时间的逆序将错误信息一步步地往前传递，当输入序列比较长时，会存在梯度爆炸和消失问题，也称为长程依赖问题。为了解决这个问题，人们对循环神经网络进行了很多的改进，其中最有效的改进方式引入门控机制(Gating Mechanism)。</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; 本博文主要介绍循环神经网络，长短期记忆网络LSTM以及门控循环单元网络GRU。</span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">1.&nbsp;前馈网络存在的问题</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202109/1027447-20210909160046250-1649070108.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">2.&nbsp;循环神经网络（Recurrent Neural Network，RNN）</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202109/1027447-20210909160115653-1784551406.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p>&nbsp;</p>
<p><img src="https://img2020.cnblogs.com/blog/1027447/202109/1027447-20210909160939055-1219097304.jpg" alt="" width="1283" height="534" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">3.&nbsp;长短期记忆神经网络（Long Short-Term Memory, LSTM）</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202109/1027447-20210909160207005-1546586688.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211102091950571-520978982.jpg" alt="" width="1102" height="372" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202109/1027447-20210909160221607-685706551.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202109/1027447-20210909160229021-1897325857.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">4.&nbsp;门控循环单元网络（Gated Recurrent Unit，GRU）</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202109/1027447-20210909160300647-1899182690.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">5.&nbsp;深层循环神经网络</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202109/1027447-20210909160325884-3482634.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202109/1027447-20210909160333704-617477360.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">6. 参考文献</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[1] 邱锡鹏，神经网络与深度学习，机械工业出版社，<a href="https://nndl.github.io/" target="_blank">https://nndl.github.io/</a>, 2020.</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[2] The Unreasonable Effectiveness of Recurrent Neural Networks <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[3] Understanding LSTM Networks -- colah's blog <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[4] [干货]深入浅出LSTM及其Python代码实现 - 知乎 <a href="https://zhuanlan.zhihu.com/p/104475016" target="_blank">https://zhuanlan.zhihu.com/p/104475016</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[5]&nbsp;一文搞懂RNN（循环神经网络）基础篇 - 知乎 <a href="https://zhuanlan.zhihu.com/p/30844905" target="_blank">https://zhuanlan.zhihu.com/p/30844905</a></span></p>