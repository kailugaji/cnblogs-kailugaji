<h1 style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif;">Meta-RL&mdash;&mdash;Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices</span></h1>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">作者：凯鲁嘎吉 - 博客园&nbsp;<a href="http://www.cnblogs.com/kailugaji/" target="_blank">http://www.cnblogs.com/kailugaji/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; 这篇博客简要回顾论文&ldquo;<a href="http://proceedings.mlr.press/v139/liu21s.html" target="_blank">Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices</a>&rdquo;，并记录一下阅读笔记，论文主要解决的问题是元强化学习中智能体如何学习探索。原先所提出的端到端优化探索与利用方法实现简单，原则上能得到最优策略，但无法摆脱鸡蛋相生问题，一旦探索没学好，利用也就得不到好结果，同时，端到端方式可能会无效探索，浪费了大量的探索来恢复与任务无关的信息，导致学不到最优行为。这篇论文在不牺牲最优性的条件下让探索与利用解耦，从而避免鸡蛋相生问题。</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; 元强化学习(meta-RL)的目标是构建智能体，该智能体能够通过利用相关任务的先前经验快速学习新任务。学习一项新任务通常需要探索以收集与任务相关的信息，并利用这些信息来解决任务。原则上，可以通过简单地最大化任务性能来学习端到端的最佳探索和利用。然而，这种meta-RL方法相当于鸡蛋相生问题，因而可能陷入局部最优解：学习探索需要良好的利用来衡量探索的效用，但学习利用需要通过探索收集信息。分开优化探索和利用的目标可以避免此问题，但先前的meta-RL探索目标会产生收集与任务无关信息的次优策略。论文通过构建一个自动识别任务相关信息的利用目标(最小化信息瓶颈Information Bottleneck)和一个仅恢复该信息的探索目标(最大化互信息)来缓解这两个问题。这避免了端到端训练中的局部最优，而不会牺牲最佳探索。从经验上看，DREAM在复杂的meta-RL问题(如稀疏奖励3D视觉导航)上明显优于现有方法。</span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">1. 基础知识</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; 包括：信息熵(Information Entropy, or Shannon Entropy)，联合熵(Joint Entropy)[对称]，条件熵(Conditional Entropy)[非对称]，互信息(Mutual Information)[对称]，相对熵(Relative Entropy, or K-L Divergence)[非对称]，互信息与相对熵的关系，信息熵，联合熵，条件熵与互信息的关系，端到端学习(End-to-End Learning)，信用/贡献度分配问题(Credit Assignment Problem，CAP)，马尔可夫性(无后效性/无记忆性)与非马尔可夫性(Markov vs Non-Markov)，鸡蛋相生问题(Chicken-and-Egg Problem)，以及探索-利用困境(Exploration-Exploitation&nbsp;Dilemma)。</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211110151604029-134980024.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211110151615960-57322449.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211110151624942-1456320783.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211110151634506-1666153485.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">2.&nbsp;Meta-Reinforcement Learning (元强化学习)</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211110151700903-327808406.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211110151710637-669000822.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">3.&nbsp;End-to-end meta-RL (端到端的元强化学习)及其问题</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211110151734175-937809993.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">4.&nbsp;Decoupled Reward-free ExplorAtion and Execution in Meta-RL (DREAM)</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211110151757986-573267649.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211110151806882-1693344588.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211110151815242-1881427495.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211110151824302-585375809.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">5.&nbsp;参考文献</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[1] Evan Z Liu, Aditi Raghunathan, Percy Liang, Chelsea Finn. Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices. ICML, pp. 6925-6935, 2021.</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Paper: <a href="http://proceedings.mlr.press/v139/liu21s.html">http://proceedings.mlr.press/v139/liu21s.html</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Code: <a href="https://github.com/ezliu/dream">https://github.com/ezliu/dream</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Slides: <a href="https://icml.cc/media/icml-2021/Slides/8991.pdf">https://icml.cc/media/icml-2021/Slides/8991.pdf</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Poster: <a href="https://docs.google.com/presentation/d/1EsDzcnYghgBNIxGCMbxEdYo6nusgUVC7DpDsusMUshE/edit?usp=sharing">https://docs.google.com/presentation/d/1EsDzcnYghgBNIxGCMbxEdYo6nusgUVC7DpDsusMUshE/edit?usp=sharing</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Blog: <a href="https://ai.stanford.edu/blog/meta-exploration/">https://ai.stanford.edu/blog/meta-exploration/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[2] CS330, Meta-RL 2: Learning to explore (Chelsea Finn), <a href="https://web.stanford.edu/class/cs330/slides/cs330_metarl2_2021.pdf">https://web.stanford.edu/class/cs330/slides/cs330_metarl2_2021.pdf</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[3] Chapter 2: Entropy and Mutual Information <a href="https://www.cs.uic.edu/pub/ECE534/WebHome/ch2.pdf">https://www.cs.uic.edu/pub/ECE534/WebHome/ch2.pdf</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[4] 邱锡鹏，神经网络与深度学习，机械工业出版社，<a href="https://nndl.github.io/">https://nndl.github.io/</a>, 2020.</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[5] &ldquo;Chicken-and-egg.&rdquo; Dictionary. <a href="https://www.learnersdictionary.com/definition/chicken-and-egg">https://www.learnersdictionary.com/definition/chicken-and-egg</a>. <a href="https://idioms.thefreedictionary.com/chicken+and+egg+problem">https://idioms.thefreedictionary.com/chicken+and+egg+problem</a>.</span></p>
<p align="left"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[6] 周志华. 机器学习. 清华大学出版社, 2016.</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[7] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy. Deep variational information bottleneck. ICLR 2017. <a href="https://openreview.net/pdf?id=HyxQzBceg">https://openreview.net/pdf?id=HyxQzBceg</a></span></p>