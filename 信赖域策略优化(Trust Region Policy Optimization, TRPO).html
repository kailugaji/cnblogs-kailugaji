<h1 style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif;">信赖域策略优化(Trust Region Policy Optimization, TRPO)</span></h1>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">作者：凯鲁嘎吉 - 博客园&nbsp;<a href="http://www.cnblogs.com/kailugaji/" target="_blank">http://www.cnblogs.com/kailugaji/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; 这篇博文是John S., Sergey L., Pieter A., Michael J., Philipp M., <a href="http://proceedings.mlr.press/v37/schulman15.pdf" target="_blank">Trust Region Policy Optimization</a>. Proceedings of the 32nd International Conference on Machine Learning, PMLR 37:1889-1897, 2015.的阅读笔记，用来介绍TRPO策略优化方法及其一些公式的推导。TRPO是一种基于策略梯度的强化学习方法，除了定理1没推导之外，其他公式的来龙去脉都进行了详细介绍，为后续进一步深入研究其他强化学习方法提供基础。更多强化学习内容，请看：<a href="https://www.cnblogs.com/kailugaji/category/2038931.html" target="_blank">随笔分类 - Reinforcement Learning</a>。</span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">1. 基础知识</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">KL散度 (Kullback&ndash;Leibler Divergence or Relative Entropy)，总变差散度(Total Variation Divergence)，以及KL散度与TV散度之间的关系(Pinsker&rsquo;s inequality)</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211010132354364-229822469.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">共轭梯度法(Conjugate Gradient Algorithm)</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211010132422606-491304455.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">新旧策略期望折扣奖励差</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211010132438235-884856427.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211010132450045-852719415.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211010132501205-1147235957.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">2. &eta;的局部近似</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211010132515280-1391029679.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211010132528667-2016857208.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211010132545602-1765460904.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2 id="1633843546113"><span style="font-family: 'comic sans ms', sans-serif;">3.&nbsp;一般性随机策略的单调提升保证</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211010132606232-1869946157.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">4.&nbsp;参数化策略的优化问题</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211010132620393-1925771759.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">5.&nbsp;Sample-Based Estimation of the Objective and Constraint</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211010132646171-1664973602.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">6.&nbsp;约束优化问题的求解</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211010132700589-1296915547.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211010132710461-2023691627.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">7. 算法总体流程</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211010132723342-49956768.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">8. 参考文献</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[1] John S., Sergey L., Pieter A., Michael J., Philipp M., <a href="http://proceedings.mlr.press/v37/schulman15.pdf" target="_blank">Trust Region Policy Optimization</a>. Proceedings of the 32nd International Conference on Machine Learning, PMLR 37:1889-1897, 2015.</span></p>
<p id="1633837396517"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[2]&nbsp;Entropy and Information Theory, Robert M. Gray, <a href="http://www.cis.jhu.edu/~bruno/s06-466/GrayIT.pdf" target="_blank">http://www.cis.jhu.edu/~bruno/s06-466/GrayIT.pdf</a>. Lemma 5.2.8 P88.</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[3]&nbsp;Su G . On Choosing and Bounding Probability Metrics. International Statistical Review, 2002, 70(3). <a href="https://arxiv.org/pdf/math/0209021.pdf" target="_blank">https://arxiv.org/pdf/math/0209021.pdf</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[4]&nbsp;Concentration inequalities: A nonasymptotic theory of independence, <a href="http://home.ustc.edu.cn/~luke2001/pdf/concentration.pdf" target="_blank">http://home.ustc.edu.cn/~luke2001/pdf/concentration.pdf</a>, Theorem 4.19, P103, Pinsker's inequality.&nbsp;</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[5]&nbsp;J. Nocedal and S. J. Wright, Numerical optimization. New York, NY: Springer (2006; Zbl 1104.65059) <a href="http://www.apmath.spbu.ru/cnsa/pdf/monograf/Numerical_Optimization2006.pdf" target="_blank">http://www.apmath.spbu.ru/cnsa/pdf/monograf/Numerical_Optimization2006.pdf</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[6]&nbsp;Kakade, Sham and Langford, John. <a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=0B4ED7B15F1CD354B29E3C39CAA5BADA?doi=10.1.1.7.7601&amp;rep=rep1&amp;type=pdf" target="_blank">Approximately optimal approximate reinforcement learning</a>. In ICML, volume 2, pp. 267 274, 2002.</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[7]&nbsp;Trust Region Policy Optimization&nbsp;<a href="https://spinningup.openai.com/en/latest/algorithms/trpo.html" target="_blank">https://spinningup.openai.com/en/latest/algorithms/trpo.html</a></span></p>