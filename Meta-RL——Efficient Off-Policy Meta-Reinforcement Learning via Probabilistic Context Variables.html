<h1 style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif;">Meta-RL&mdash;&mdash;Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables</span></h1>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">作者：凯鲁嘎吉 - 博客园&nbsp;<a href="http://www.cnblogs.com/kailugaji/" target="_blank">http://www.cnblogs.com/kailugaji/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; 这篇博客是&ldquo;<a href="http://proceedings.mlr.press/v97/rakelly19a.html" target="_blank">Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables</a>&rdquo;的简要阅读笔记，围绕如何从过去学习的任务中针对新的任务获取有效的信息以及如何对新任务的不确定性做出更准确的判断这两个问题，论文通过一个任务编码器来学习任务的表征(概率上下文变量z)，融合软演员评论员算法(Soft Actor-Critic, SAC)与变分推断KL损失，提出一种异策略元强化学习算法，通过分离任务推断与智能体学习过程来提高元学习中任务的学习样本利用效率。</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; 深度强化学习算法需要大量的经验来学习单个任务。虽然元强化学习(meta-RL)算法可以让智能体从少量经验中学习新技能，但一些主要的挑战阻碍了它们的实用性。目前的方法严重依赖于同策略经验，限制了它们的样本效率。在适应新任务时，它们也缺乏推理任务不确定性的机制，这限制了它们在稀疏奖励问题上的有效性。本文通过开发一种异策略元强化学习算法来解决这些挑战，所提算法(Probabilistic Embeddings for Actor-critic meta-RL, PEARL)将任务推理和控制分离开来。算法对潜在的任务变量进行在线概率滤波，从少量的经验中推断出如何解决新任务。这种概率解释使得后验采样能够用于结构化和高效的探索。论文证明了如何将这些任务变量与异策略强化学习算法集成，以实现元训练和自适应效率。所提方法在样本效率和在几个元强化学习基准上的渐近性能上都比以前的算法高出20-100倍。</span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">1. 基础知识</span></h2>
<h3><span style="font-family: 'comic sans ms', sans-serif;">1.1 传统的强化学习 vs 元强化学习 (Standard RL vs Meta-RL)</span></h3>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211123130041707-484443003.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h3><span style="font-family: 'comic sans ms', sans-serif;">1.2 同策略 vs 异策略 (On-Policy vs Off-Policy)</span></h3>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211123130111891-1205822129.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h3><span style="font-family: 'comic sans ms', sans-serif;">1.3 软演员评论员算法(Soft Actor-Critic, SAC)</span></h3>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211123130127464-109830206.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">2. 概率潜在上下文 (Probabilistic Latent Context)</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211123130158807-383957745.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211123130206731-1974948175.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211123130215476-2115200317.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">3. Probabilistic Embeddings for Actor-critic meta-RL (PEARL)</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211123130227517-776594346.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211123130253793-1553317135.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p><img src="https://img2020.cnblogs.com/blog/1027447/202111/1027447-20211123130304807-1307461606.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">4. 参考文献</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[1] Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, Deirdre Quillen. Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables. ICML, 2019.</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Paper:&nbsp;<a href="http://proceedings.mlr.press/v97/rakelly19a.html">http://proceedings.mlr.press/v97/rakelly19a.html</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Code: <a href="https://github.com/katerakelly/oyster">https://github.com/katerakelly/oyster</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Slides: <a href="https://icml.cc/media/Slides/icml/2019/hallb(12-11-00)-12-12-15-4607-efficient_off-p.pdf">https://icml.cc/media/Slides/icml/2019/hallb(12-11-00)-12-12-15-4607-efficient_off-p.pdf</a> and <a href="https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring20/slides/cs885-efficient-off-policy-meta-reinforcement-learning.pdf">https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring20/slides/cs885-efficient-off-policy-meta-reinforcement-learning.pdf</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Video: <a href="https://youtube.videoken.com/embed/D0UmVbbJxS8?tocitem=100">https://youtube.videoken.com/embed/D0UmVbbJxS8?tocitem=100</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[2] CS285 Meta-Learning <a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa20/static/slides/lec-22.pdf">http://rail.eecs.berkeley.edu/deeprlcourse-fa20/static/slides/lec-22.pdf</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[3] CS330 Meta-RL <a href="http://cs330.stanford.edu/fall2019/slides/Exploration%20in%20Meta-RL.pdf">http://cs330.stanford.edu/fall2019/slides/Exploration%20in%20Meta-RL.pdf</a> and <a href="https://web.stanford.edu/class/cs330/slides/cs330_metarl2_2021.pdf">https://web.stanford.edu/class/cs330/slides/cs330_metarl2_2021.pdf</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[4] PEARL &mdash; Probabilistic Embedding for Actor-critic RL | Zero <a href="https://xlnwel.github.io/blog/reinforcement%20learning/PEARL/">https://xlnwel.github.io/blog/reinforcement%20learning/PEARL/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[5] RL&mdash;&mdash;Deep Reinforcement Learning amidst Continual/Lifelong Structured Non-Stationarity &ndash; 凯鲁嘎吉 &ndash; 博客园 <a href="https://www.cnblogs.com/kailugaji/p/15562366.html">https://www.cnblogs.com/kailugaji/p/15562366.html</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[6] 变分推断与变分自编码器 &ndash; 凯鲁嘎吉 &ndash; 博客园 <a href="https://www.cnblogs.com/kailugaji/p/12463966.html">https://www.cnblogs.com/kailugaji/p/12463966.html</a></span></p>