<h1 style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif;">元学习&mdash;&mdash;从MAML到MAML++</span></h1>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">作者：凯鲁嘎吉 - 博客园&nbsp;<a href="http://www.cnblogs.com/kailugaji/" target="_blank">http://www.cnblogs.com/kailugaji/</a></span></p>
<p><span class="tgt" style="font-family: 'comic sans ms', sans-serif; font-size: 16px;" data-section="0" data-sentence="0" data-group="0-0">&nbsp; &nbsp; Few-shot learning领域最近有了实质性的进展。<span class="tgt" data-section="0" data-sentence="1" data-group="0-1">这些进步大多来自于将few-shot learning作为元学习问题。<span class="tgt highlight" data-section="0" data-sentence="2" data-group="0-2"><a href="http://proceedings.mlr.press/v70/finn17a" target="_blank">Model-Agnostic Meta-Learning</a> (MAML)是目前利用元学习进行few-shot learning的最佳方法之一。<span class="tgt" data-section="0" data-sentence="3" data-group="0-3">MAML简单，优雅，功能强大，但是它有很多问题，比如对神经网络结构非常敏感，经常导致训练时不稳定，<span class="tgt" data-section="0" data-sentence="4" data-group="0-4">需要费力的超参数搜索来稳定训练和实现高泛化，并且在训练和推理时间上都非常昂贵的计算。<span class="tgt" data-section="0" data-sentence="5" data-group="0-5">在文"<a href="https://openreview.net/pdf?id=HJGven05Y7" target="_blank">How to train your MAML</a>"中，对MAML进行了各种改进，不仅稳定了系统，而且大幅度提高了MAML的泛化性能、收敛速度和计算开销。所提方法<span class="tgt" data-section="0" data-sentence="6" data-group="0-6">称之为MAML++。本博文首先介绍什么是元学习，经典的Model-Agnostic Meta-Learning的定义与执行过程，进而说明MAML面临的缺点与挑战，针对这些问题，进行相应改进，从而得到MAML++。</span></span></span></span></span></span></span></p>
<h2><span class="tgt" style="font-family: 'comic sans ms', sans-serif;" data-section="0" data-sentence="0" data-group="0-0"><span class="tgt" data-section="0" data-sentence="1" data-group="0-1"><span class="tgt highlight" data-section="0" data-sentence="2" data-group="0-2"><span class="tgt" data-section="0" data-sentence="3" data-group="0-3"><span class="tgt" data-section="0" data-sentence="4" data-group="0-4"><span class="tgt" data-section="0" data-sentence="5" data-group="0-5"><span class="tgt" data-section="0" data-sentence="6" data-group="0-6">1. Meta Learning (Learn to Learn)</span></span></span></span></span></span></span></h2>
<p><span class="tgt" style="font-family: 'comic sans ms', sans-serif;" data-section="0" data-sentence="0" data-group="0-0"><span class="tgt" data-section="0" data-sentence="1" data-group="0-1"><span class="tgt highlight" data-section="0" data-sentence="2" data-group="0-2"><span class="tgt" data-section="0" data-sentence="3" data-group="0-3"><span class="tgt" data-section="0" data-sentence="4" data-group="0-4"><span class="tgt" data-section="0" data-sentence="5" data-group="0-5"><span class="tgt" data-section="0" data-sentence="6" data-group="0-6"><img src="https://img2020.cnblogs.com/blog/1027447/202107/1027447-20210708102155263-183491835.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></span></span></span></span></span></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">2. Black-Box Adaption vs Optimization-Based Approach</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202107/1027447-20210708102210117-794468960.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">3. MAML</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202107/1027447-20210708151756900-1232211194.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">MAML Computation Graph</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202107/1027447-20210708103038515-1347748682.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">4. MAML Problems</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202107/1027447-20210708102228127-1345734453.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">5. MAML++</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202107/1027447-20210708102236495-446089691.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202107/1027447-20210708102244182-1543089984.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202107/1027447-20210708102250812-2027356207.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;"><img src="https://img2020.cnblogs.com/blog/1027447/202107/1027447-20210708102256893-1725254631.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">MAML with Multi-Step Loss Computation Graph</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202107/1027447-20210708103209298-50323573.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">6. 参考文献</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[1]&nbsp;Finn, C., Abbeel, P. &amp; Levine, S. <a href="http://proceedings.mlr.press/v70/finn17a" target="_blank">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a>. ICML 2017. Code:&nbsp;<a href="https://github.com/cbfinn/maml" target="_blank">https://github.com/cbfinn/maml</a>,&nbsp;<a href="https://github.com/dragen1860/MAML-Pytorch" target="_blank">https://github.com/dragen1860/MAML-Pytorch</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Finn个人主页：<a href="https://ai.stanford.edu/~cbfinn/" target="_blank">https://ai.stanford.edu/~cbfinn/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[2]&nbsp;Antoniou, A.,&nbsp; Edwards, H., &amp;&nbsp; Storkey, A. <a href="https://openreview.net/pdf?id=HJGven05Y7" target="_blank">How to train your MAML</a>. ICLR 2019. Code:&nbsp;<a href="https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch" target="_blank">https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[3]&nbsp;How to train your MAML: A step by step approach &middot; BayesWatch <a href="https://www.bayeswatch.com/2018/11/30/HTYM/" target="_blank">https://www.bayeswatch.com/2018/11/30/HTYM/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[4]&nbsp;CS 330: Deep Multi-Task and Meta Learning <a href="http://web.stanford.edu/class/cs330/" target="_blank">http://web.stanford.edu/class/cs330/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[5]&nbsp;Meta-Learning: Learning to Learn Fast <a href="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html" target="_blank">https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html</a></span></p>