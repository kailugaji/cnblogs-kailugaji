<h1 style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif;">Deep Reinforcement Learning Hands-On&mdash;&mdash;Policy Gradients &ndash; an Alternative</span></h1>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">ä½œè€…ï¼šå‡¯é²å˜å‰ - åšå®¢å›­&nbsp;<a href="http://www.cnblogs.com/kailugaji/" rel="noopener" target="_blank">http://www.cnblogs.com/kailugaji/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">æ›´å¤šè¯·çœ‹ï¼šReinforcement Learning - éšç¬”åˆ†ç±» - å‡¯é²å˜å‰ - åšå®¢å›­&nbsp;<a href="https://www.cnblogs.com/kailugaji/category/2038931.html" rel="noopener" target="_blank">https://www.cnblogs.com/kailugaji/category/2038931.html</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">æœ¬æ–‡ä»£ç ä¸‹è½½ï¼š<a href="https://github.com/kailugaji/Hands-on-Reinforcement-Learning/tree/main/03%20Policy%20Gradients" target="_blank">https://github.com/kailugaji/Hands-on-Reinforcement-Learning/tree/main/03%20Policy%20Gradients</a><br /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; è¿™ä¸€ç¯‡åšæ–‡å‚è€ƒäº†ä¹¦ç›®ã€Š<a href="https://gitee.com/devilmaycry812839668/deep-reinforcement-learning-hands-on-second-edition" rel="noopener" target="_blank">Deep Reinforcement Learning Hands-On Second Edition</a>ã€‹ç¬¬11ç« å†…å®¹ï¼Œä¸»è¦ä»‹ç»åŸºäºç­–ç•¥å‡½æ•°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬ï¼šREINFORCEã€å¸¦åŸºå‡†çº¿çš„REINFORCEç®—æ³•(REINFORCE with Baseline)ä»¥åŠç­–ç•¥æ¢¯åº¦æ–¹æ³•(å¼•å…¥entropy bonusæ¥å¢åŠ æ¢ç´¢ï¼Œå¼•å…¥åŸºå‡†çº¿æ¥è§£å†³æ–¹å·®å¤§è®­ç»ƒä¸ç¨³å®šé—®é¢˜)ã€‚å…·ä½“ç†è®ºçŸ¥è¯†ï¼Œè¯·å‚è§ï¼š<a href="https://www.cnblogs.com/kailugaji/p/15354491.html#_lab2_0_2" target="_blank">å¼ºåŒ–å­¦ä¹ (Reinforcement Learning) - å‡¯é²å˜å‰ - åšå®¢å›­</a>ã€‚ä¸ºäº†ä¸DQNç®—æ³•ä½œå¯¹æ¯”ï¼Œé¦–å…ˆç”¨Pythoné‡æ–°å®ç°äº†DQNç®—æ³•(æ¶æ„ä¸æ˜¯æ ‡å‡†çš„ä¸‰å·ç§¯ä¸¤å…¨è¿æ¥ï¼Œè€Œæ˜¯ç®€åŒ–ä¸ºä¸¤å…¨è¿æ¥) 01_cartpole_dqn.pyï¼Œæ¥ç€åˆ†åˆ«å®ç°åŸºäºç­–ç•¥å‡½æ•°çš„ä¸‰ä¸ªå¼ºåŒ–å­¦ä¹ æ–¹æ³•02_cartpole_reinforce.pyï¼Œ03_cartpole_reinforce_baseline.pyï¼Œä¸04_cartpole_pg.pyï¼Œè¿˜æœ‰ä¸€ä¸ªæ”¹è¿›çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•è§ï¼š<a href="https://github.com/kailugaji/Hands-on-Reinforcement-Learning/blob/main/03%20Policy%20Gradients/05_pong_pg.py">https://github.com/kailugaji/Hands-on-Reinforcement-Learning/blob/main/03%20Policy%20Gradients/05_pong_pg.py</a>ï¼Œè¯¥ç®—æ³•ç”¨åˆ°äº†æ¢¯åº¦è£å‰ªç­–ç•¥ï¼Œå¤šç¯å¢ƒå¹¶è¡Œå®ç°ï¼Œé‡‡ç”¨ç§»åŠ¨å¹³å‡æ³•å®šä¹‰åŸºå‡†çº¿ï¼Œç”±äºæŸäº›å¤–åœ¨å› ç´ ï¼Œç»“æœå¹¶æœªåˆ—å‡ºï¼Œå¯è‡ªè¡Œè¿è¡Œã€‚06_cartpole_pg.pyæ˜¯04_cartpole_pg.pyçš„æ‰©å±•ï¼Œå”¯ä¸€çš„åŒºåˆ«åœ¨äºbaselineå¯ä»¥ç”±ç”¨æˆ·é€‰æ‹©åŠ æˆ–è€…ä¸åŠ ï¼Œå»ºè®®ä½¿ç”¨06_cartpole_pg.pyã€‚</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; å€¼å‡½æ•°ä¸ç­–ç•¥å‡½æ•°æ–¹æ³•çš„åŒºåˆ«ï¼šåŸºäºå€¼å‡½æ•°çš„æ–¹æ³•ç½‘ç»œè¾“å‡ºçš„æ˜¯Qå€¼ï¼Œè€ŒåŸºäºç­–ç•¥å‡½æ•°çš„æ–¹æ³•ç½‘ç»œè¾“å‡ºçš„æ˜¯åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ(ä»£ç é‡Œæ˜¯è¾“å‡ºQå€¼ï¼Œå†è¿›è¡Œsoftmaxå¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ)ï¼Œé€šè¿‡éšæœºé‡‡æ ·ï¼Œå¾—åˆ°æœ€ç»ˆçš„åŠ¨ä½œã€‚</span></p>
<p><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220301085922860-467629329.jpg" alt="" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">1.&nbsp;01_cartpole_dqn.py</span></h2>
<h3><span style="font-family: 'comic sans ms', sans-serif;">1.1 ç¨‹åº</span></h3>
<div class="cnblogs_Highlighter">
<pre class="brush:python;gutter:true;"><span style="font-family: 'comic sans ms', sans-serif;">#!/usr/bin/env python3
# -*- coding=utf-8 -*-
# DQN
# The CartPole example
# https://www.cnblogs.com/kailugaji/
import gym
import ptan
import numpy as np
from tensorboardX import SummaryWriter

import torch
import torch.nn as nn
import torch.optim as optim

GAMMA = 0.99 # æŠ˜æ‰£ç‡ 0.9
LEARNING_RATE = 0.01 # å­¦ä¹ ç‡ 5e-3
BATCH_SIZE = 16 # ä¸€æ‰¹xxä¸ªæ ·æœ¬ 16

EPSILON_START = 1.0 # epsilonå› å­
EPSILON_STOP = 0.02
EPSILON_STEPS = 5000

REPLAY_BUFFER = 5000 # ç»éªŒå›æ”¾æ± å®¹é‡

# æ„å»ºç½‘ç»œ
class DQN(nn.Module):
    def __init__(self, input_size, n_actions):
        # input_sizeï¼šè¾“å…¥çŠ¶æ€ç»´åº¦ï¼Œhidden_sizeï¼šéšå±‚ç»´åº¦=128ï¼Œn_actionsï¼šè¾“å‡ºåŠ¨ä½œç»´åº¦
        super(DQN, self).__init__()

        self.net = nn.Sequential(
            nn.Linear(input_size, 128), # å…¨è¿æ¥å±‚ï¼Œéšå±‚é¢„è®¾ä¸º128ç»´åº¦
            nn.ReLU(),
            nn.Linear(128, n_actions) # å…¨è¿æ¥å±‚
        )

    def forward(self, x):
        return self.net(x)

# ç›®æ ‡ç½‘ç»œ r + gamma * max Q(s, a)
def calc_target(net, local_reward, next_state):
    if next_state is None:
        return local_reward
    state_v = torch.tensor([next_state], dtype=torch.float32)
    next_q_v = net(state_v) # å°†æœ€åçš„çŠ¶æ€è¾“å…¥ç½‘ç»œï¼Œå¾—åˆ°Q(s, a)
    best_q = next_q_v.max(dim=1)[0].item() # æ‰¾æœ€å¤§çš„Q
    return local_reward + GAMMA * best_q
    # r + gamma * max Q(s, a)


if __name__ == "__main__":
    env = gym.make("CartPole-v0") # åˆ›å»ºæ¸¸æˆç¯å¢ƒ
    writer = SummaryWriter(comment="-cartpole-dqn")

    net = DQN(env.observation_space.shape[0], env.action_space.n) # 4(çŠ¶æ€)-&gt;128-&gt;2(åŠ¨ä½œ)
    # print(net)

    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=EPSILON_START)
    # epsilon-greedy action selectorï¼Œåˆå§‹epsilon=1
    agent = ptan.agent.DQNAgent(net, selector, preprocessor=ptan.agent.float32_preprocessor)
    # DQNAgentï¼šç¦»æ•£
    # dqn_modelæ¢æˆè‡ªå®šä¹‰çš„DQNæ¨¡å‹ï¼Œ4(çŠ¶æ€)-&gt;128-&gt;2(åŠ¨ä½œ)
    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)
    # è¿”å›è¿è¡Œè®°å½•ä»¥ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œè¾“å‡ºæ ¼å¼ä¸ºï¼š(state, action, reward, last_state)
    replay_buffer = ptan.experience.ExperienceReplayBuffer(exp_source, REPLAY_BUFFER)
    # ç»éªŒå›æ”¾æ± ï¼Œæ„å»ºbufferï¼Œå®¹é‡ä¸º1000ï¼Œå½“å‰æ²¡ä¸œè¥¿ï¼Œlen(buffer) = 0
    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE) # Adamä¼˜åŒ–
    mse_loss = nn.MSELoss() # MSE loss

    total_rewards = [] # the total rewards for the episodes
    step_idx = 0 # è¿­ä»£æ¬¡æ•°/è½®æ•°
    done_episodes = 0 # å±€æ•°ï¼Œå‡ å±€æ¸¸æˆ

    while True:
        step_idx += 1
        selector.epsilon = max(EPSILON_STOP, EPSILON_START - step_idx / EPSILON_STEPS)
        # epsilonéšè¿­ä»£æ­¥æ•°çš„è¡°å‡ç­–ç•¥
        replay_buffer.populate(1)  # ä»ç¯å¢ƒä¸­è·å–ä¸€ä¸ªæ–°æ ·æœ¬

        if len(replay_buffer) &lt; BATCH_SIZE: # bufferé‡Œé¢è¿˜æ²¡è¶…è¿‡BATCH_SIZEä¸ªæ ·æœ¬
            continue

        # sample batch
        batch = replay_buffer.sample(BATCH_SIZE) # ä»bufferé‡Œé¢å‡åŒ€æŠ½æ ·ä¸€ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬ï¼Œä¸€æ‰¹BATCH_SIZEä¸ªæ ·æœ¬
        batch_states = [exp.state for exp in batch]
        batch_actions = [exp.action for exp in batch]
        batch_targets = [calc_target(net, exp.reward, exp.last_state)
                         for exp in batch] # r + gamma * max Q(s, a)
        # train
        optimizer.zero_grad()
        states_v = torch.FloatTensor(batch_states)
        net_q_v = net(states_v) # è¾“å…¥çŠ¶æ€ï¼Œè¾“å‡ºQ(s, a)å€¼
        target_q = net_q_v.data.numpy().copy() # copyç½‘ç»œå‚æ•°
        target_q[range(BATCH_SIZE), batch_actions] = batch_targets
        target_q_v = torch.tensor(target_q) # r + gamma * max Q(s, a)
        loss_v = mse_loss(net_q_v, target_q_v) # min L = (r + gamma * max Q(s', a') - Q(s, a))^2
        loss_v.backward()
        optimizer.step()

        # handle new rewards
        new_rewards = exp_source.pop_total_rewards() # è¿”å›ä¸€å±€æ¸¸æˆè¿‡åçš„total_rewords
        if new_rewards:
            done_episodes += 1
            reward = new_rewards[0]
            total_rewards.append(reward) # å‰done_episodeså±€æ¸¸æˆçš„å¥–åŠ±ä¹‹å’Œ
            mean_rewards = float(np.mean(total_rewards[-100:])) # total_rewards/done_episodes
            print("ç¬¬%dæ¬¡: ç¬¬%då±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º%6.2f, å¹³å‡å¥–åŠ±ä¸º%6.2f, epsilonä¸º%.2f" % (step_idx, done_episodes, reward, mean_rewards, selector.epsilon))
            writer.add_scalar("reward", reward, step_idx)
            writer.add_scalar("reward_100", mean_rewards, step_idx)
            writer.add_scalar("epsilon", selector.epsilon, step_idx)
            writer.add_scalar("episodes", done_episodes, step_idx)
            if mean_rewards &gt; 50: # æœ€å¤§æœŸæœ›å¥–åŠ±é˜ˆå€¼ï¼Œåªæœ‰å½“å¹³å‡å¥–åŠ± &gt; 50æ—¶æ‰ç»“æŸæ¸¸æˆ
                print("ç»è¿‡%dè½®å®Œæˆ%då±€æ¸¸æˆ!" % (step_idx, done_episodes))
                break
    writer.close()</span></pre>
</div>
<h3><span style="font-family: 'comic sans ms', sans-serif;">1.2 ç»“æœ</span></h3>
<div class="cnblogs_Highlighter">
<pre class="brush:python;gutter:true;"><span style="font-family: 'comic sans ms', sans-serif;">ç¬¬17æ¬¡: ç¬¬1å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 16.00, å¹³å‡å¥–åŠ±ä¸º 16.00, epsilonä¸º1.00
ç¬¬40æ¬¡: ç¬¬2å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 23.00, å¹³å‡å¥–åŠ±ä¸º 19.50, epsilonä¸º0.99
ç¬¬67æ¬¡: ç¬¬3å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 27.00, å¹³å‡å¥–åŠ±ä¸º 22.00, epsilonä¸º0.99
ç¬¬105æ¬¡: ç¬¬4å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 38.00, å¹³å‡å¥–åŠ±ä¸º 26.00, epsilonä¸º0.98
ç¬¬142æ¬¡: ç¬¬5å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 37.00, å¹³å‡å¥–åŠ±ä¸º 28.20, epsilonä¸º0.97
ç¬¬161æ¬¡: ç¬¬6å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 19.00, å¹³å‡å¥–åŠ±ä¸º 26.67, epsilonä¸º0.97
ç¬¬186æ¬¡: ç¬¬7å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 25.00, å¹³å‡å¥–åŠ±ä¸º 26.43, epsilonä¸º0.96
ç¬¬197æ¬¡: ç¬¬8å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 11.00, å¹³å‡å¥–åŠ±ä¸º 24.50, epsilonä¸º0.96
ç¬¬219æ¬¡: ç¬¬9å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 22.00, å¹³å‡å¥–åŠ±ä¸º 24.22, epsilonä¸º0.96
ç¬¬249æ¬¡: ç¬¬10å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 30.00, å¹³å‡å¥–åŠ±ä¸º 24.80, epsilonä¸º0.95
ç¬¬262æ¬¡: ç¬¬11å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 13.00, å¹³å‡å¥–åŠ±ä¸º 23.73, epsilonä¸º0.95
ç¬¬283æ¬¡: ç¬¬12å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 21.00, å¹³å‡å¥–åŠ±ä¸º 23.50, epsilonä¸º0.94
ç¬¬307æ¬¡: ç¬¬13å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 24.00, å¹³å‡å¥–åŠ±ä¸º 23.54, epsilonä¸º0.94
ç¬¬333æ¬¡: ç¬¬14å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 26.00, å¹³å‡å¥–åŠ±ä¸º 23.71, epsilonä¸º0.93
ç¬¬348æ¬¡: ç¬¬15å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 15.00, å¹³å‡å¥–åŠ±ä¸º 23.13, epsilonä¸º0.93
ç¬¬387æ¬¡: ç¬¬16å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 39.00, å¹³å‡å¥–åŠ±ä¸º 24.12, epsilonä¸º0.92
ç¬¬411æ¬¡: ç¬¬17å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 24.00, å¹³å‡å¥–åŠ±ä¸º 24.12, epsilonä¸º0.92
ç¬¬429æ¬¡: ç¬¬18å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 18.00, å¹³å‡å¥–åŠ±ä¸º 23.78, epsilonä¸º0.91
ç¬¬469æ¬¡: ç¬¬19å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 40.00, å¹³å‡å¥–åŠ±ä¸º 24.63, epsilonä¸º0.91
ç¬¬481æ¬¡: ç¬¬20å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 12.00, å¹³å‡å¥–åŠ±ä¸º 24.00, epsilonä¸º0.90
ç¬¬493æ¬¡: ç¬¬21å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 12.00, å¹³å‡å¥–åŠ±ä¸º 23.43, epsilonä¸º0.90
ç¬¬513æ¬¡: ç¬¬22å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 20.00, å¹³å‡å¥–åŠ±ä¸º 23.27, epsilonä¸º0.90
ç¬¬527æ¬¡: ç¬¬23å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 14.00, å¹³å‡å¥–åŠ±ä¸º 22.87, epsilonä¸º0.89
ç¬¬578æ¬¡: ç¬¬24å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 51.00, å¹³å‡å¥–åŠ±ä¸º 24.04, epsilonä¸º0.88
ç¬¬592æ¬¡: ç¬¬25å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 14.00, å¹³å‡å¥–åŠ±ä¸º 23.64, epsilonä¸º0.88
ç¬¬604æ¬¡: ç¬¬26å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 12.00, å¹³å‡å¥–åŠ±ä¸º 23.19, epsilonä¸º0.88
ç¬¬632æ¬¡: ç¬¬27å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 28.00, å¹³å‡å¥–åŠ±ä¸º 23.37, epsilonä¸º0.87
ç¬¬664æ¬¡: ç¬¬28å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 32.00, å¹³å‡å¥–åŠ±ä¸º 23.68, epsilonä¸º0.87
ç¬¬700æ¬¡: ç¬¬29å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 36.00, å¹³å‡å¥–åŠ±ä¸º 24.10, epsilonä¸º0.86
ç¬¬711æ¬¡: ç¬¬30å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 11.00, å¹³å‡å¥–åŠ±ä¸º 23.67, epsilonä¸º0.86
ç¬¬745æ¬¡: ç¬¬31å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 34.00, å¹³å‡å¥–åŠ±ä¸º 24.00, epsilonä¸º0.85
ç¬¬760æ¬¡: ç¬¬32å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 15.00, å¹³å‡å¥–åŠ±ä¸º 23.72, epsilonä¸º0.85
ç¬¬774æ¬¡: ç¬¬33å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 14.00, å¹³å‡å¥–åŠ±ä¸º 23.42, epsilonä¸º0.85
ç¬¬798æ¬¡: ç¬¬34å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 24.00, å¹³å‡å¥–åŠ±ä¸º 23.44, epsilonä¸º0.84
ç¬¬817æ¬¡: ç¬¬35å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 19.00, å¹³å‡å¥–åŠ±ä¸º 23.31, epsilonä¸º0.84
ç¬¬829æ¬¡: ç¬¬36å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 12.00, å¹³å‡å¥–åŠ±ä¸º 23.00, epsilonä¸º0.83
ç¬¬862æ¬¡: ç¬¬37å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 33.00, å¹³å‡å¥–åŠ±ä¸º 23.27, epsilonä¸º0.83
ç¬¬881æ¬¡: ç¬¬38å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 19.00, å¹³å‡å¥–åŠ±ä¸º 23.16, epsilonä¸º0.82
ç¬¬906æ¬¡: ç¬¬39å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 25.00, å¹³å‡å¥–åŠ±ä¸º 23.21, epsilonä¸º0.82
ç¬¬929æ¬¡: ç¬¬40å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 23.00, å¹³å‡å¥–åŠ±ä¸º 23.20, epsilonä¸º0.81
ç¬¬963æ¬¡: ç¬¬41å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 34.00, å¹³å‡å¥–åŠ±ä¸º 23.46, epsilonä¸º0.81
ç¬¬987æ¬¡: ç¬¬42å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 24.00, å¹³å‡å¥–åŠ±ä¸º 23.48, epsilonä¸º0.80
ç¬¬1017æ¬¡: ç¬¬43å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 30.00, å¹³å‡å¥–åŠ±ä¸º 23.63, epsilonä¸º0.80
ç¬¬1062æ¬¡: ç¬¬44å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 45.00, å¹³å‡å¥–åŠ±ä¸º 24.11, epsilonä¸º0.79
ç¬¬1131æ¬¡: ç¬¬45å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 69.00, å¹³å‡å¥–åŠ±ä¸º 25.11, epsilonä¸º0.77
ç¬¬1163æ¬¡: ç¬¬46å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 32.00, å¹³å‡å¥–åŠ±ä¸º 25.26, epsilonä¸º0.77
ç¬¬1173æ¬¡: ç¬¬47å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 10.00, å¹³å‡å¥–åŠ±ä¸º 24.94, epsilonä¸º0.77
ç¬¬1190æ¬¡: ç¬¬48å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 17.00, å¹³å‡å¥–åŠ±ä¸º 24.77, epsilonä¸º0.76
ç¬¬1222æ¬¡: ç¬¬49å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 32.00, å¹³å‡å¥–åŠ±ä¸º 24.92, epsilonä¸º0.76
ç¬¬1243æ¬¡: ç¬¬50å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 21.00, å¹³å‡å¥–åŠ±ä¸º 24.84, epsilonä¸º0.75
ç¬¬1265æ¬¡: ç¬¬51å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 22.00, å¹³å‡å¥–åŠ±ä¸º 24.78, epsilonä¸º0.75
ç¬¬1277æ¬¡: ç¬¬52å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 12.00, å¹³å‡å¥–åŠ±ä¸º 24.54, epsilonä¸º0.74
ç¬¬1292æ¬¡: ç¬¬53å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 15.00, å¹³å‡å¥–åŠ±ä¸º 24.36, epsilonä¸º0.74
ç¬¬1325æ¬¡: ç¬¬54å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 33.00, å¹³å‡å¥–åŠ±ä¸º 24.52, epsilonä¸º0.73
ç¬¬1341æ¬¡: ç¬¬55å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 16.00, å¹³å‡å¥–åŠ±ä¸º 24.36, epsilonä¸º0.73
ç¬¬1370æ¬¡: ç¬¬56å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 29.00, å¹³å‡å¥–åŠ±ä¸º 24.45, epsilonä¸º0.73
ç¬¬1403æ¬¡: ç¬¬57å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 33.00, å¹³å‡å¥–åŠ±ä¸º 24.60, epsilonä¸º0.72
ç¬¬1422æ¬¡: ç¬¬58å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 19.00, å¹³å‡å¥–åŠ±ä¸º 24.50, epsilonä¸º0.72
ç¬¬1471æ¬¡: ç¬¬59å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 49.00, å¹³å‡å¥–åŠ±ä¸º 24.92, epsilonä¸º0.71
ç¬¬1483æ¬¡: ç¬¬60å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 12.00, å¹³å‡å¥–åŠ±ä¸º 24.70, epsilonä¸º0.70
ç¬¬1594æ¬¡: ç¬¬61å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º111.00, å¹³å‡å¥–åŠ±ä¸º 26.11, epsilonä¸º0.68
ç¬¬1621æ¬¡: ç¬¬62å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 27.00, å¹³å‡å¥–åŠ±ä¸º 26.13, epsilonä¸º0.68
ç¬¬1637æ¬¡: ç¬¬63å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 16.00, å¹³å‡å¥–åŠ±ä¸º 25.97, epsilonä¸º0.67
ç¬¬1684æ¬¡: ç¬¬64å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 47.00, å¹³å‡å¥–åŠ±ä¸º 26.30, epsilonä¸º0.66
ç¬¬1805æ¬¡: ç¬¬65å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º121.00, å¹³å‡å¥–åŠ±ä¸º 27.75, epsilonä¸º0.64
ç¬¬1855æ¬¡: ç¬¬66å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 50.00, å¹³å‡å¥–åŠ±ä¸º 28.09, epsilonä¸º0.63
ç¬¬1904æ¬¡: ç¬¬67å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 49.00, å¹³å‡å¥–åŠ±ä¸º 28.40, epsilonä¸º0.62
ç¬¬1924æ¬¡: ç¬¬68å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 20.00, å¹³å‡å¥–åŠ±ä¸º 28.28, epsilonä¸º0.62
ç¬¬1967æ¬¡: ç¬¬69å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 43.00, å¹³å‡å¥–åŠ±ä¸º 28.49, epsilonä¸º0.61
ç¬¬2042æ¬¡: ç¬¬70å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 75.00, å¹³å‡å¥–åŠ±ä¸º 29.16, epsilonä¸º0.59
ç¬¬2075æ¬¡: ç¬¬71å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 33.00, å¹³å‡å¥–åŠ±ä¸º 29.21, epsilonä¸º0.58
ç¬¬2110æ¬¡: ç¬¬72å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 35.00, å¹³å‡å¥–åŠ±ä¸º 29.29, epsilonä¸º0.58
ç¬¬2127æ¬¡: ç¬¬73å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 17.00, å¹³å‡å¥–åŠ±ä¸º 29.12, epsilonä¸º0.57
ç¬¬2161æ¬¡: ç¬¬74å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 34.00, å¹³å‡å¥–åŠ±ä¸º 29.19, epsilonä¸º0.57
ç¬¬2219æ¬¡: ç¬¬75å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 58.00, å¹³å‡å¥–åŠ±ä¸º 29.57, epsilonä¸º0.56
ç¬¬2238æ¬¡: ç¬¬76å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 19.00, å¹³å‡å¥–åŠ±ä¸º 29.43, epsilonä¸º0.55
ç¬¬2300æ¬¡: ç¬¬77å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 62.00, å¹³å‡å¥–åŠ±ä¸º 29.86, epsilonä¸º0.54
ç¬¬2348æ¬¡: ç¬¬78å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 48.00, å¹³å‡å¥–åŠ±ä¸º 30.09, epsilonä¸º0.53
ç¬¬2454æ¬¡: ç¬¬79å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º106.00, å¹³å‡å¥–åŠ±ä¸º 31.05, epsilonä¸º0.51
ç¬¬2527æ¬¡: ç¬¬80å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 73.00, å¹³å‡å¥–åŠ±ä¸º 31.57, epsilonä¸º0.49
ç¬¬2590æ¬¡: ç¬¬81å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 63.00, å¹³å‡å¥–åŠ±ä¸º 31.96, epsilonä¸º0.48
ç¬¬2642æ¬¡: ç¬¬82å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 52.00, å¹³å‡å¥–åŠ±ä¸º 32.21, epsilonä¸º0.47
ç¬¬2723æ¬¡: ç¬¬83å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 81.00, å¹³å‡å¥–åŠ±ä¸º 32.80, epsilonä¸º0.46
ç¬¬2824æ¬¡: ç¬¬84å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º101.00, å¹³å‡å¥–åŠ±ä¸º 33.61, epsilonä¸º0.44
ç¬¬2865æ¬¡: ç¬¬85å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 41.00, å¹³å‡å¥–åŠ±ä¸º 33.69, epsilonä¸º0.43
ç¬¬2957æ¬¡: ç¬¬86å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 92.00, å¹³å‡å¥–åŠ±ä¸º 34.37, epsilonä¸º0.41
ç¬¬3067æ¬¡: ç¬¬87å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º110.00, å¹³å‡å¥–åŠ±ä¸º 35.24, epsilonä¸º0.39
ç¬¬3124æ¬¡: ç¬¬88å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 57.00, å¹³å‡å¥–åŠ±ä¸º 35.49, epsilonä¸º0.38
ç¬¬3188æ¬¡: ç¬¬89å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 64.00, å¹³å‡å¥–åŠ±ä¸º 35.81, epsilonä¸º0.36
ç¬¬3274æ¬¡: ç¬¬90å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 86.00, å¹³å‡å¥–åŠ±ä¸º 36.37, epsilonä¸º0.35
ç¬¬3334æ¬¡: ç¬¬91å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 60.00, å¹³å‡å¥–åŠ±ä¸º 36.63, epsilonä¸º0.33
ç¬¬3424æ¬¡: ç¬¬92å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 90.00, å¹³å‡å¥–åŠ±ä¸º 37.21, epsilonä¸º0.32
ç¬¬3503æ¬¡: ç¬¬93å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 79.00, å¹³å‡å¥–åŠ±ä¸º 37.66, epsilonä¸º0.30
ç¬¬3587æ¬¡: ç¬¬94å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 84.00, å¹³å‡å¥–åŠ±ä¸º 38.15, epsilonä¸º0.28
ç¬¬3787æ¬¡: ç¬¬95å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º200.00, å¹³å‡å¥–åŠ±ä¸º 39.85, epsilonä¸º0.24
ç¬¬3879æ¬¡: ç¬¬96å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 92.00, å¹³å‡å¥–åŠ±ä¸º 40.40, epsilonä¸º0.22
ç¬¬3965æ¬¡: ç¬¬97å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 86.00, å¹³å‡å¥–åŠ±ä¸º 40.87, epsilonä¸º0.21
ç¬¬4070æ¬¡: ç¬¬98å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º105.00, å¹³å‡å¥–åŠ±ä¸º 41.52, epsilonä¸º0.19
ç¬¬4146æ¬¡: ç¬¬99å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 76.00, å¹³å‡å¥–åŠ±ä¸º 41.87, epsilonä¸º0.17
ç¬¬4199æ¬¡: ç¬¬100å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 53.00, å¹³å‡å¥–åŠ±ä¸º 41.98, epsilonä¸º0.16
ç¬¬4250æ¬¡: ç¬¬101å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 51.00, å¹³å‡å¥–åŠ±ä¸º 42.33, epsilonä¸º0.15
ç¬¬4321æ¬¡: ç¬¬102å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 71.00, å¹³å‡å¥–åŠ±ä¸º 42.81, epsilonä¸º0.14
ç¬¬4418æ¬¡: ç¬¬103å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 97.00, å¹³å‡å¥–åŠ±ä¸º 43.51, epsilonä¸º0.12
ç¬¬4537æ¬¡: ç¬¬104å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º119.00, å¹³å‡å¥–åŠ±ä¸º 44.32, epsilonä¸º0.09
ç¬¬4680æ¬¡: ç¬¬105å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º143.00, å¹³å‡å¥–åŠ±ä¸º 45.38, epsilonä¸º0.06
ç¬¬4749æ¬¡: ç¬¬106å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 69.00, å¹³å‡å¥–åŠ±ä¸º 45.88, epsilonä¸º0.05
ç¬¬4888æ¬¡: ç¬¬107å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º139.00, å¹³å‡å¥–åŠ±ä¸º 47.02, epsilonä¸º0.02
ç¬¬4941æ¬¡: ç¬¬108å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 53.00, å¹³å‡å¥–åŠ±ä¸º 47.44, epsilonä¸º0.02
ç¬¬5106æ¬¡: ç¬¬109å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º165.00, å¹³å‡å¥–åŠ±ä¸º 48.87, epsilonä¸º0.02
ç¬¬5162æ¬¡: ç¬¬110å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 56.00, å¹³å‡å¥–åŠ±ä¸º 49.13, epsilonä¸º0.02
ç¬¬5212æ¬¡: ç¬¬111å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 50.00, å¹³å‡å¥–åŠ±ä¸º 49.50, epsilonä¸º0.02
ç¬¬5286æ¬¡: ç¬¬112å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 74.00, å¹³å‡å¥–åŠ±ä¸º 50.03, epsilonä¸º0.02
ç»è¿‡5286è½®å®Œæˆ112å±€æ¸¸æˆ!</span></pre>
</div>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;"><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220301090022153-1089824192.png" alt="" width="768" height="301" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">epsilonè¡°å‡æ›²çº¿</span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">2.&nbsp;02_cartpole_reinforce.py</span></h2>
<h3><span style="font-family: 'comic sans ms', sans-serif;">2.1 ç¨‹åº</span></h3>
<div class="cnblogs_Highlighter">
<pre class="brush:python;gutter:true;"><span style="font-family: 'comic sans ms', sans-serif;">#!/usr/bin/env python3
# -*- coding=utf-8 -*-
# Policy Gradients&mdash;&mdash;REINFORCE
# The CartPole example
# https://www.cnblogs.com/kailugaji/
import gym
import ptan
import numpy as np
from tensorboardX import SummaryWriter

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

GAMMA = 0.99 # æŠ˜æ‰£ç‡
LEARNING_RATE = 0.01 # å­¦ä¹ ç‡
EPISODES_TO_TRAIN = 4 # how many complete episodes we will use for training

# æ„å»ºç½‘ç»œ
class PGN(nn.Module):
    def __init__(self, input_size, n_actions):
        # input_sizeï¼šè¾“å…¥çŠ¶æ€ç»´åº¦ï¼Œhidden_sizeï¼šéšå±‚ç»´åº¦=128ï¼Œn_actionsï¼šè¾“å‡ºåŠ¨ä½œç»´åº¦
        super(PGN, self).__init__()

        self.net = nn.Sequential(
            nn.Linear(input_size, 128), # å…¨è¿æ¥å±‚ï¼Œéšå±‚é¢„è®¾ä¸º128ç»´åº¦
            nn.ReLU(),
            nn.Linear(128, n_actions) # å…¨è¿æ¥å±‚
        )

    def forward(self, x):
        return self.net(x) # æœªç”¨softmax

# Calculate the reward from the end of the local reward list.
# Indeed, the last step of the episode will have a total reward equal to its local reward.
# The step before the last will have the total reward of r(ğ‘¡&minus;1) + gamma * r(ğ‘¡)
# sum(gamma^t * reward)
def calc_qvals(rewards):
    res = []
    sum_r = 0.0
    for r in reversed(rewards): # reversed: è¿”å›åå‘çš„è¿­ä»£å™¨å¯¹è±¡
        sum_r *= GAMMA # gamma * r(ğ‘¡), r(t): the total reward for the previous steps
        sum_r += r # r(ğ‘¡&minus;1) + gamma * r(ğ‘¡), r(t-1): the local reward
        res.append(sum_r) # the discounted total reward
    return list(reversed(res)) # a list of rewards for the whole episode


if __name__ == "__main__":
    env = gym.make("CartPole-v0") # åˆ›å»ºæ¸¸æˆç¯å¢ƒ
    writer = SummaryWriter(comment="-cartpole-reinforce")

    net = PGN(env.observation_space.shape[0], env.action_space.n) # 4(çŠ¶æ€)-&gt;128-&gt;2(åŠ¨ä½œ)
    # print(net)

    agent = ptan.agent.PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor, apply_softmax=True)
    # PolicyAgentï¼šè¿ç»­
    # make a decision about actions for every observation (ä¾æ¦‚ç‡)
    # apply_softmax=Trueï¼šç½‘ç»œè¾“å‡ºå…ˆç»è¿‡softmaxè½¬åŒ–æˆæ¦‚ç‡ï¼Œå†ä»è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸­è¿›è¡ŒéšæœºæŠ½æ ·
    # float32_preprocessorï¼šreturns the observation as float64 instead of the float32 required by PyTorch
    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)
    # è¿”å›è¿è¡Œè®°å½•ä»¥ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œè¾“å‡ºæ ¼å¼ä¸ºï¼š(state, action, reward, last_state)
    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE) # Adamä¼˜åŒ–

    total_rewards = [] # the total rewards for the episodes
    step_idx = 0 # è¿­ä»£æ¬¡æ•°/è½®æ•°
    done_episodes = 0 # å±€æ•°ï¼Œå‡ å±€æ¸¸æˆ

    batch_episodes = 0
    batch_states, batch_actions, batch_qvals = [], [], []
    cur_rewards = [] # local rewards for the episode being currently played

    for step_idx, exp in enumerate(exp_source): # (state, action, reward, last_state)
        batch_states.append(exp.state) # çŠ¶æ€
        batch_actions.append(int(exp.action)) # åŠ¨ä½œ
        cur_rewards.append(exp.reward) # å³æ—¶å¥–åŠ±

        if exp.last_state is None: # ä¸€å±€æ¸¸æˆç»“æŸ
            batch_qvals.extend(calc_qvals(cur_rewards)) # the discounted total rewards
            cur_rewards.clear()
            batch_episodes += 1

        # handle new rewards
        new_rewards = exp_source.pop_total_rewards() # è¿”å›ä¸€å±€æ¸¸æˆè¿‡åçš„total_rewords
        if new_rewards:
            done_episodes += 1 # æ¸¸æˆå±€æ•°(å›åˆæ•°)
            reward = new_rewards[0]
            total_rewards.append(reward) # the total rewards for the episodes
            mean_rewards = float(np.mean(total_rewards[-100:])) # å¹³å‡å¥–åŠ±
            print("ç¬¬%dæ¬¡: ç¬¬%då±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º%6.2f, å¹³å‡å¥–åŠ±ä¸º%6.2f" % (step_idx, done_episodes, reward, mean_rewards))
            writer.add_scalar("reward", reward, step_idx)
            writer.add_scalar("reward_100", mean_rewards, step_idx)
            writer.add_scalar("episodes", done_episodes, step_idx)
            if mean_rewards &gt; 50: # æœ€å¤§æœŸæœ›å¥–åŠ±é˜ˆå€¼ï¼Œåªæœ‰å½“mean_rewards &gt; 50æ—¶æ‰ç»“æŸæ¸¸æˆ
                print("ç»è¿‡%dè½®å®Œæˆ%då±€æ¸¸æˆ!" % (step_idx, done_episodes))
                break

        if batch_episodes &lt; EPISODES_TO_TRAIN: # how many complete episodes we will use for training
            continue

        # train
        optimizer.zero_grad()
        states_v = torch.FloatTensor(batch_states)
        batch_actions_t = torch.LongTensor(batch_actions)
        batch_qvals_v = torch.FloatTensor(batch_qvals) # the discounted batch total rewards

        logits_v = net(states_v) # è¾“å…¥çŠ¶æ€ï¼Œè¾“å‡ºQ(s, a)å€¼
        log_prob_v = F.log_softmax(logits_v, dim=1) # è¾“å‡ºåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡åˆ†å¸ƒ log &pi;(s, a)
        log_prob_actions_v = batch_qvals_v * log_prob_v[range(len(batch_states)), batch_actions_t]
        # max sum(gamma^t * reward) * log &pi;(s, a)
        loss_v = -log_prob_actions_v.mean() # min

        loss_v.backward()
        optimizer.step()

        batch_episodes = 0
        batch_states.clear()
        batch_actions.clear()
        batch_qvals.clear()

    writer.close()</span></pre>
</div>
<h3><span style="font-family: 'comic sans ms', sans-serif;">2.2 ç»“æœ</span></h3>
<div class="cnblogs_Highlighter">
<pre class="brush:python;gutter:true;"><span style="font-family: 'comic sans ms', sans-serif;">ç¬¬21æ¬¡: ç¬¬1å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 21.00, å¹³å‡å¥–åŠ±ä¸º 21.00
ç¬¬65æ¬¡: ç¬¬2å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 44.00, å¹³å‡å¥–åŠ±ä¸º 32.50
ç¬¬78æ¬¡: ç¬¬3å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 13.00, å¹³å‡å¥–åŠ±ä¸º 26.00
ç¬¬91æ¬¡: ç¬¬4å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 13.00, å¹³å‡å¥–åŠ±ä¸º 22.75
ç¬¬112æ¬¡: ç¬¬5å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 21.00, å¹³å‡å¥–åŠ±ä¸º 22.40
ç¬¬131æ¬¡: ç¬¬6å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 19.00, å¹³å‡å¥–åŠ±ä¸º 21.83
ç¬¬158æ¬¡: ç¬¬7å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 27.00, å¹³å‡å¥–åŠ±ä¸º 22.57
ç¬¬173æ¬¡: ç¬¬8å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 15.00, å¹³å‡å¥–åŠ±ä¸º 21.62
ç¬¬199æ¬¡: ç¬¬9å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 26.00, å¹³å‡å¥–åŠ±ä¸º 22.11
ç¬¬212æ¬¡: ç¬¬10å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 13.00, å¹³å‡å¥–åŠ±ä¸º 21.20
ç¬¬240æ¬¡: ç¬¬11å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 28.00, å¹³å‡å¥–åŠ±ä¸º 21.82
ç¬¬267æ¬¡: ç¬¬12å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 27.00, å¹³å‡å¥–åŠ±ä¸º 22.25
ç¬¬289æ¬¡: ç¬¬13å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 22.00, å¹³å‡å¥–åŠ±ä¸º 22.23
ç¬¬310æ¬¡: ç¬¬14å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 21.00, å¹³å‡å¥–åŠ±ä¸º 22.14
ç¬¬335æ¬¡: ç¬¬15å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 25.00, å¹³å‡å¥–åŠ±ä¸º 22.33
ç¬¬377æ¬¡: ç¬¬16å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 42.00, å¹³å‡å¥–åŠ±ä¸º 23.56
ç¬¬386æ¬¡: ç¬¬17å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º  9.00, å¹³å‡å¥–åŠ±ä¸º 22.71
ç¬¬442æ¬¡: ç¬¬18å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 56.00, å¹³å‡å¥–åŠ±ä¸º 24.56
ç¬¬463æ¬¡: ç¬¬19å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 21.00, å¹³å‡å¥–åŠ±ä¸º 24.37
ç¬¬510æ¬¡: ç¬¬20å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 47.00, å¹³å‡å¥–åŠ±ä¸º 25.50
ç¬¬572æ¬¡: ç¬¬21å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 62.00, å¹³å‡å¥–åŠ±ä¸º 27.24
ç¬¬627æ¬¡: ç¬¬22å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 55.00, å¹³å‡å¥–åŠ±ä¸º 28.50
ç¬¬671æ¬¡: ç¬¬23å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 44.00, å¹³å‡å¥–åŠ±ä¸º 29.17
ç¬¬761æ¬¡: ç¬¬24å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 90.00, å¹³å‡å¥–åŠ±ä¸º 31.71
ç¬¬797æ¬¡: ç¬¬25å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 36.00, å¹³å‡å¥–åŠ±ä¸º 31.88
ç¬¬873æ¬¡: ç¬¬26å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 76.00, å¹³å‡å¥–åŠ±ä¸º 33.58
ç¬¬932æ¬¡: ç¬¬27å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 59.00, å¹³å‡å¥–åŠ±ä¸º 34.52
ç¬¬965æ¬¡: ç¬¬28å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 33.00, å¹³å‡å¥–åŠ±ä¸º 34.46
ç¬¬1017æ¬¡: ç¬¬29å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 52.00, å¹³å‡å¥–åŠ±ä¸º 35.07
ç¬¬1047æ¬¡: ç¬¬30å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 30.00, å¹³å‡å¥–åŠ±ä¸º 34.90
ç¬¬1121æ¬¡: ç¬¬31å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 74.00, å¹³å‡å¥–åŠ±ä¸º 36.16
ç¬¬1139æ¬¡: ç¬¬32å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 18.00, å¹³å‡å¥–åŠ±ä¸º 35.59
ç¬¬1161æ¬¡: ç¬¬33å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 22.00, å¹³å‡å¥–åŠ±ä¸º 35.18
ç¬¬1329æ¬¡: ç¬¬34å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º168.00, å¹³å‡å¥–åŠ±ä¸º 39.09
ç¬¬1361æ¬¡: ç¬¬35å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 32.00, å¹³å‡å¥–åŠ±ä¸º 38.89
ç¬¬1414æ¬¡: ç¬¬36å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 53.00, å¹³å‡å¥–åŠ±ä¸º 39.28
ç¬¬1431æ¬¡: ç¬¬37å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 17.00, å¹³å‡å¥–åŠ±ä¸º 38.68
ç¬¬1519æ¬¡: ç¬¬38å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 88.00, å¹³å‡å¥–åŠ±ä¸º 39.97
ç¬¬1552æ¬¡: ç¬¬39å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 33.00, å¹³å‡å¥–åŠ±ä¸º 39.79
ç¬¬1568æ¬¡: ç¬¬40å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 16.00, å¹³å‡å¥–åŠ±ä¸º 39.20
ç¬¬1614æ¬¡: ç¬¬41å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 46.00, å¹³å‡å¥–åŠ±ä¸º 39.37
ç¬¬1674æ¬¡: ç¬¬42å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 60.00, å¹³å‡å¥–åŠ±ä¸º 39.86
ç¬¬1750æ¬¡: ç¬¬43å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 76.00, å¹³å‡å¥–åŠ±ä¸º 40.70
ç¬¬1802æ¬¡: ç¬¬44å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 52.00, å¹³å‡å¥–åŠ±ä¸º 40.95
ç¬¬1854æ¬¡: ç¬¬45å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 52.00, å¹³å‡å¥–åŠ±ä¸º 41.20
ç¬¬1897æ¬¡: ç¬¬46å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 43.00, å¹³å‡å¥–åŠ±ä¸º 41.24
ç¬¬1994æ¬¡: ç¬¬47å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 97.00, å¹³å‡å¥–åŠ±ä¸º 42.43
ç¬¬2051æ¬¡: ç¬¬48å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 57.00, å¹³å‡å¥–åŠ±ä¸º 42.73
ç¬¬2105æ¬¡: ç¬¬49å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 54.00, å¹³å‡å¥–åŠ±ä¸º 42.96
ç¬¬2169æ¬¡: ç¬¬50å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 64.00, å¹³å‡å¥–åŠ±ä¸º 43.38
ç¬¬2202æ¬¡: ç¬¬51å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 33.00, å¹³å‡å¥–åŠ±ä¸º 43.18
ç¬¬2220æ¬¡: ç¬¬52å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 18.00, å¹³å‡å¥–åŠ±ä¸º 42.69
ç¬¬2254æ¬¡: ç¬¬53å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 34.00, å¹³å‡å¥–åŠ±ä¸º 42.53
ç¬¬2310æ¬¡: ç¬¬54å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 56.00, å¹³å‡å¥–åŠ±ä¸º 42.78
ç¬¬2367æ¬¡: ç¬¬55å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 57.00, å¹³å‡å¥–åŠ±ä¸º 43.04
ç¬¬2450æ¬¡: ç¬¬56å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 83.00, å¹³å‡å¥–åŠ±ä¸º 43.75
ç¬¬2496æ¬¡: ç¬¬57å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 46.00, å¹³å‡å¥–åŠ±ä¸º 43.79
ç¬¬2532æ¬¡: ç¬¬58å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 36.00, å¹³å‡å¥–åŠ±ä¸º 43.66
ç¬¬2554æ¬¡: ç¬¬59å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 22.00, å¹³å‡å¥–åŠ±ä¸º 43.29
ç¬¬2599æ¬¡: ç¬¬60å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 45.00, å¹³å‡å¥–åŠ±ä¸º 43.32
ç¬¬2630æ¬¡: ç¬¬61å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 31.00, å¹³å‡å¥–åŠ±ä¸º 43.11
ç¬¬2659æ¬¡: ç¬¬62å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 29.00, å¹³å‡å¥–åŠ±ä¸º 42.89
ç¬¬2708æ¬¡: ç¬¬63å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 49.00, å¹³å‡å¥–åŠ±ä¸º 42.98
ç¬¬2772æ¬¡: ç¬¬64å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 64.00, å¹³å‡å¥–åŠ±ä¸º 43.31
ç¬¬2801æ¬¡: ç¬¬65å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 29.00, å¹³å‡å¥–åŠ±ä¸º 43.09
ç¬¬2830æ¬¡: ç¬¬66å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 29.00, å¹³å‡å¥–åŠ±ä¸º 42.88
ç¬¬2878æ¬¡: ç¬¬67å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 48.00, å¹³å‡å¥–åŠ±ä¸º 42.96
ç¬¬2928æ¬¡: ç¬¬68å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 50.00, å¹³å‡å¥–åŠ±ä¸º 43.06
ç¬¬2947æ¬¡: ç¬¬69å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 19.00, å¹³å‡å¥–åŠ±ä¸º 42.71
ç¬¬2973æ¬¡: ç¬¬70å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 26.00, å¹³å‡å¥–åŠ±ä¸º 42.47
ç¬¬3004æ¬¡: ç¬¬71å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 31.00, å¹³å‡å¥–åŠ±ä¸º 42.31
ç¬¬3050æ¬¡: ç¬¬72å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 46.00, å¹³å‡å¥–åŠ±ä¸º 42.36
ç¬¬3073æ¬¡: ç¬¬73å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 23.00, å¹³å‡å¥–åŠ±ä¸º 42.10
ç¬¬3149æ¬¡: ç¬¬74å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 76.00, å¹³å‡å¥–åŠ±ä¸º 42.55
ç¬¬3193æ¬¡: ç¬¬75å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 44.00, å¹³å‡å¥–åŠ±ä¸º 42.57
ç¬¬3216æ¬¡: ç¬¬76å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 23.00, å¹³å‡å¥–åŠ±ä¸º 42.32
ç¬¬3272æ¬¡: ç¬¬77å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 56.00, å¹³å‡å¥–åŠ±ä¸º 42.49
ç¬¬3327æ¬¡: ç¬¬78å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 55.00, å¹³å‡å¥–åŠ±ä¸º 42.65
ç¬¬3401æ¬¡: ç¬¬79å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 74.00, å¹³å‡å¥–åŠ±ä¸º 43.05
ç¬¬3457æ¬¡: ç¬¬80å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 56.00, å¹³å‡å¥–åŠ±ä¸º 43.21
ç¬¬3486æ¬¡: ç¬¬81å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 29.00, å¹³å‡å¥–åŠ±ä¸º 43.04
ç¬¬3531æ¬¡: ç¬¬82å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 45.00, å¹³å‡å¥–åŠ±ä¸º 43.06
ç¬¬3583æ¬¡: ç¬¬83å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 52.00, å¹³å‡å¥–åŠ±ä¸º 43.17
ç¬¬3600æ¬¡: ç¬¬84å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 17.00, å¹³å‡å¥–åŠ±ä¸º 42.86
ç¬¬3661æ¬¡: ç¬¬85å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 61.00, å¹³å‡å¥–åŠ±ä¸º 43.07
ç¬¬3688æ¬¡: ç¬¬86å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 27.00, å¹³å‡å¥–åŠ±ä¸º 42.88
ç¬¬3751æ¬¡: ç¬¬87å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 63.00, å¹³å‡å¥–åŠ±ä¸º 43.11
ç¬¬3825æ¬¡: ç¬¬88å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 74.00, å¹³å‡å¥–åŠ±ä¸º 43.47
ç¬¬3973æ¬¡: ç¬¬89å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º148.00, å¹³å‡å¥–åŠ±ä¸º 44.64
ç¬¬4033æ¬¡: ç¬¬90å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 60.00, å¹³å‡å¥–åŠ±ä¸º 44.81
ç¬¬4103æ¬¡: ç¬¬91å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 70.00, å¹³å‡å¥–åŠ±ä¸º 45.09
ç¬¬4160æ¬¡: ç¬¬92å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 57.00, å¹³å‡å¥–åŠ±ä¸º 45.22
ç¬¬4192æ¬¡: ç¬¬93å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 32.00, å¹³å‡å¥–åŠ±ä¸º 45.08
ç¬¬4255æ¬¡: ç¬¬94å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 63.00, å¹³å‡å¥–åŠ±ä¸º 45.27
ç¬¬4326æ¬¡: ç¬¬95å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 71.00, å¹³å‡å¥–åŠ±ä¸º 45.54
ç¬¬4397æ¬¡: ç¬¬96å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 71.00, å¹³å‡å¥–åŠ±ä¸º 45.80
ç¬¬4453æ¬¡: ç¬¬97å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 56.00, å¹³å‡å¥–åŠ±ä¸º 45.91
ç¬¬4483æ¬¡: ç¬¬98å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 30.00, å¹³å‡å¥–åŠ±ä¸º 45.74
ç¬¬4521æ¬¡: ç¬¬99å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 38.00, å¹³å‡å¥–åŠ±ä¸º 45.67
ç¬¬4608æ¬¡: ç¬¬100å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 87.00, å¹³å‡å¥–åŠ±ä¸º 46.08
ç¬¬4690æ¬¡: ç¬¬101å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 82.00, å¹³å‡å¥–åŠ±ä¸º 46.69
ç¬¬4737æ¬¡: ç¬¬102å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 47.00, å¹³å‡å¥–åŠ±ä¸º 46.72
ç¬¬4801æ¬¡: ç¬¬103å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 64.00, å¹³å‡å¥–åŠ±ä¸º 47.23
ç¬¬4893æ¬¡: ç¬¬104å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 92.00, å¹³å‡å¥–åŠ±ä¸º 48.02
ç¬¬4981æ¬¡: ç¬¬105å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 88.00, å¹³å‡å¥–åŠ±ä¸º 48.69
ç¬¬5038æ¬¡: ç¬¬106å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 57.00, å¹³å‡å¥–åŠ±ä¸º 49.07
ç¬¬5109æ¬¡: ç¬¬107å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 71.00, å¹³å‡å¥–åŠ±ä¸º 49.51
ç¬¬5138æ¬¡: ç¬¬108å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 29.00, å¹³å‡å¥–åŠ±ä¸º 49.65
ç¬¬5189æ¬¡: ç¬¬109å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 51.00, å¹³å‡å¥–åŠ±ä¸º 49.90
ç¬¬5278æ¬¡: ç¬¬110å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 89.00, å¹³å‡å¥–åŠ±ä¸º 50.66
ç»è¿‡5278è½®å®Œæˆ110å±€æ¸¸æˆ!</span></pre>
</div>
<h2><span style="font-family: 'comic sans ms', sans-serif;">3.&nbsp;03_cartpole_reinforce_baseline.py</span></h2>
<h3><span style="font-family: 'comic sans ms', sans-serif;">3.1 ç¨‹åº</span></h3>
<div class="cnblogs_Highlighter">
<pre class="brush:python;gutter:true;"><span style="font-family: 'comic sans ms', sans-serif;">#!/usr/bin/env python3
# -*- coding=utf-8 -*-
# Policy Gradients&mdash;&mdash;å¸¦åŸºå‡†çº¿çš„REINFORCEç®—æ³•(REINFORCE with Baseline)
# REINFORCEç®—æ³•ç¼ºç‚¹ï¼šä¸åŒè·¯å¾„ä¹‹é—´çš„æ–¹å·®å¤§ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®š
# å¼•å…¥ä¸€æ§åˆ¶å˜é‡sum(gamma^t * reward)çš„æœŸæœ›ï¼šE(sum(gamma^t * reward))ï¼Œä»¥å‡å°æ–¹å·®
# Baselineï¼šthe mean of the discounted rewards
# The CartPole example
# https://www.cnblogs.com/kailugaji/
import gym
import ptan
import numpy as np
from tensorboardX import SummaryWriter

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

GAMMA = 0.99 # æŠ˜æ‰£ç‡
LEARNING_RATE = 0.01 # å­¦ä¹ ç‡
EPISODES_TO_TRAIN = 4 # how many complete episodes we will use for training

# æ„å»ºç½‘ç»œ
class PGN(nn.Module):
    def __init__(self, input_size, n_actions):
        # input_sizeï¼šè¾“å…¥çŠ¶æ€ç»´åº¦ï¼Œhidden_sizeï¼šéšå±‚ç»´åº¦=128ï¼Œn_actionsï¼šè¾“å‡ºåŠ¨ä½œç»´åº¦
        super(PGN, self).__init__()

        self.net = nn.Sequential(
            nn.Linear(input_size, 128), # å…¨è¿æ¥å±‚ï¼Œéšå±‚é¢„è®¾ä¸º128ç»´åº¦
            nn.ReLU(),
            nn.Linear(128, n_actions) # å…¨è¿æ¥å±‚
        )

    def forward(self, x):
        return self.net(x) # æœªç”¨softmax

# ä¸ä¸Šä¸€ä¸ªç¨‹åºå”¯ä¸€çš„ä¸åŒåœ¨è¿™ï¼ï¼ï¼
# sum(gamma^t * reward) - E(sum(gamma^t * reward))
def calc_qvals(rewards):
    res = []
    sum_r = 0.0
    for r in reversed(rewards):
        sum_r *= GAMMA
        sum_r += r
        res.append(sum_r)
    res = list(reversed(res))
    mean_q = np.mean(res)
    return [q - mean_q for q in res]


if __name__ == "__main__":
    env = gym.make("CartPole-v0") # åˆ›å»ºæ¸¸æˆç¯å¢ƒ
    writer = SummaryWriter(comment="-cartpole-reinforce-baseline")

    net = PGN(env.observation_space.shape[0], env.action_space.n) # 4(çŠ¶æ€)-&gt;128-&gt;2(åŠ¨ä½œ)
    # print(net)

    agent = ptan.agent.PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor,apply_softmax=True)
    # PolicyAgentï¼šè¿ç»­
    # make a decision about actions for every observation (ä¾æ¦‚ç‡)
    # apply_softmax=Trueï¼šç½‘ç»œè¾“å‡ºå…ˆç»è¿‡softmaxè½¬åŒ–æˆæ¦‚ç‡ï¼Œå†ä»è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸­è¿›è¡ŒéšæœºæŠ½æ ·
    # float32_preprocessorï¼šreturns the observation as float64 instead of the float32 required by PyTorch
    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)
    # è¿”å›è¿è¡Œè®°å½•ä»¥ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œè¾“å‡ºæ ¼å¼ä¸ºï¼š(state, action, reward, last_state)
    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE) # Adamä¼˜åŒ–

    total_rewards = [] # å‰å‡ å±€çš„å¥–åŠ±ä¹‹å’Œ
    step_idx = 0 # è¿­ä»£æ¬¡æ•°/è½®æ•°
    done_episodes = 0 # å±€æ•°ï¼Œå‡ å±€æ¸¸æˆ

    batch_episodes = 0
    batch_states, batch_actions, batch_qvals = [], [], []
    cur_states, cur_actions, cur_rewards = [], [], []

    for step_idx, exp in enumerate(exp_source): # (state, action, reward, last_state)
        cur_states.append(exp.state) # çŠ¶æ€
        cur_actions.append(int(exp.action)) # åŠ¨ä½œ
        cur_rewards.append(exp.reward) # å³æ—¶å¥–åŠ±

        if exp.last_state is None: # ä¸€å±€æ¸¸æˆç»“æŸ
            batch_states.extend(cur_states)
            batch_actions.extend(cur_actions)
            batch_qvals.extend(calc_qvals(cur_rewards))
            cur_states.clear()
            cur_actions.clear()
            cur_rewards.clear()
            batch_episodes += 1

        # handle new rewards
        new_rewards = exp_source.pop_total_rewards() # è¿”å›ä¸€å±€æ¸¸æˆè¿‡åçš„total_rewords
        if new_rewards:
            done_episodes += 1 # æ¸¸æˆå±€æ•°(å›åˆæ•°)
            reward = new_rewards[0]
            total_rewards.append(reward) # the total rewards for the episodes
            mean_rewards = float(np.mean(total_rewards[-100:])) # å¹³å‡å¥–åŠ±
            print("ç¬¬%dæ¬¡: ç¬¬%då±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º%6.2f, å¹³å‡å¥–åŠ±ä¸º%6.2f" % (step_idx, done_episodes, reward, mean_rewards))
            writer.add_scalar("reward", reward, step_idx)
            writer.add_scalar("reward_100", mean_rewards, step_idx)
            writer.add_scalar("episodes", done_episodes, step_idx)
            if mean_rewards &gt; 50: # æœ€å¤§æœŸæœ›å¥–åŠ±é˜ˆå€¼ï¼Œåªæœ‰å½“mean_rewards &gt; 50æ—¶æ‰ç»“æŸæ¸¸æˆ
                print("ç»è¿‡%dè½®å®Œæˆ%då±€æ¸¸æˆ!" % (step_idx, done_episodes))
                break

        if batch_episodes &lt; EPISODES_TO_TRAIN: # how many complete episodes we will use for training
            continue

        states_v = torch.FloatTensor(batch_states)
        batch_actions_t = torch.LongTensor(batch_actions)
        batch_qvals_v = torch.FloatTensor(batch_qvals)

        # train
        optimizer.zero_grad()
        logits_v = net(states_v) # è¾“å…¥çŠ¶æ€ï¼Œè¾“å‡ºQ(s, a)å€¼
        log_prob_v = F.log_softmax(logits_v, dim=1) # è¾“å‡ºåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡åˆ†å¸ƒ log &pi;(s, a)
        log_prob_actions_v = batch_qvals_v * log_prob_v[range(len(batch_states)), batch_actions_t]
        # max (sum(gamma^t * reward) - E(sum(gamma^t * reward))) * log &pi;(s, a)
        loss_v = -log_prob_actions_v.mean() # min

        loss_v.backward()
        optimizer.step()

        batch_episodes = 0
        batch_states.clear()
        batch_actions.clear()
        batch_qvals.clear()

    writer.close()</span></pre>
</div>
<h3><span style="font-family: 'comic sans ms', sans-serif;">3.2 ç»“æœ</span></h3>
<div class="cnblogs_Highlighter">
<pre class="brush:python;gutter:true;"><span style="font-family: 'comic sans ms', sans-serif;">ç¬¬14æ¬¡: ç¬¬1å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 14.00, å¹³å‡å¥–åŠ±ä¸º 14.00
ç¬¬35æ¬¡: ç¬¬2å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 21.00, å¹³å‡å¥–åŠ±ä¸º 17.50
ç¬¬63æ¬¡: ç¬¬3å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 28.00, å¹³å‡å¥–åŠ±ä¸º 21.00
ç¬¬84æ¬¡: ç¬¬4å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 21.00, å¹³å‡å¥–åŠ±ä¸º 21.00
ç¬¬115æ¬¡: ç¬¬5å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 31.00, å¹³å‡å¥–åŠ±ä¸º 23.00
ç¬¬131æ¬¡: ç¬¬6å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 16.00, å¹³å‡å¥–åŠ±ä¸º 21.83
ç¬¬146æ¬¡: ç¬¬7å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 15.00, å¹³å‡å¥–åŠ±ä¸º 20.86
ç¬¬180æ¬¡: ç¬¬8å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 34.00, å¹³å‡å¥–åŠ±ä¸º 22.50
ç¬¬222æ¬¡: ç¬¬9å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 42.00, å¹³å‡å¥–åŠ±ä¸º 24.67
ç¬¬260æ¬¡: ç¬¬10å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 38.00, å¹³å‡å¥–åŠ±ä¸º 26.00
ç¬¬273æ¬¡: ç¬¬11å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 13.00, å¹³å‡å¥–åŠ±ä¸º 24.82
ç¬¬300æ¬¡: ç¬¬12å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 27.00, å¹³å‡å¥–åŠ±ä¸º 25.00
ç¬¬324æ¬¡: ç¬¬13å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 24.00, å¹³å‡å¥–åŠ±ä¸º 24.92
ç¬¬346æ¬¡: ç¬¬14å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 22.00, å¹³å‡å¥–åŠ±ä¸º 24.71
ç¬¬376æ¬¡: ç¬¬15å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 30.00, å¹³å‡å¥–åŠ±ä¸º 25.07
ç¬¬445æ¬¡: ç¬¬16å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 69.00, å¹³å‡å¥–åŠ±ä¸º 27.81
ç¬¬520æ¬¡: ç¬¬17å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 75.00, å¹³å‡å¥–åŠ±ä¸º 30.59
ç¬¬603æ¬¡: ç¬¬18å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 83.00, å¹³å‡å¥–åŠ±ä¸º 33.50
ç¬¬640æ¬¡: ç¬¬19å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 37.00, å¹³å‡å¥–åŠ±ä¸º 33.68
ç¬¬715æ¬¡: ç¬¬20å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 75.00, å¹³å‡å¥–åŠ±ä¸º 35.75
ç¬¬775æ¬¡: ç¬¬21å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 60.00, å¹³å‡å¥–åŠ±ä¸º 36.90
ç¬¬798æ¬¡: ç¬¬22å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 23.00, å¹³å‡å¥–åŠ±ä¸º 36.27
ç¬¬822æ¬¡: ç¬¬23å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 24.00, å¹³å‡å¥–åŠ±ä¸º 35.74
ç¬¬840æ¬¡: ç¬¬24å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 18.00, å¹³å‡å¥–åŠ±ä¸º 35.00
ç¬¬872æ¬¡: ç¬¬25å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 32.00, å¹³å‡å¥–åŠ±ä¸º 34.88
ç¬¬922æ¬¡: ç¬¬26å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 50.00, å¹³å‡å¥–åŠ±ä¸º 35.46
ç¬¬1016æ¬¡: ç¬¬27å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 94.00, å¹³å‡å¥–åŠ±ä¸º 37.63
ç¬¬1037æ¬¡: ç¬¬28å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 21.00, å¹³å‡å¥–åŠ±ä¸º 37.04
ç¬¬1083æ¬¡: ç¬¬29å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 46.00, å¹³å‡å¥–åŠ±ä¸º 37.34
ç¬¬1107æ¬¡: ç¬¬30å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 24.00, å¹³å‡å¥–åŠ±ä¸º 36.90
ç¬¬1153æ¬¡: ç¬¬31å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 46.00, å¹³å‡å¥–åŠ±ä¸º 37.19
ç¬¬1212æ¬¡: ç¬¬32å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 59.00, å¹³å‡å¥–åŠ±ä¸º 37.88
ç¬¬1245æ¬¡: ç¬¬33å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 33.00, å¹³å‡å¥–åŠ±ä¸º 37.73
ç¬¬1280æ¬¡: ç¬¬34å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 35.00, å¹³å‡å¥–åŠ±ä¸º 37.65
ç¬¬1359æ¬¡: ç¬¬35å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 79.00, å¹³å‡å¥–åŠ±ä¸º 38.83
ç¬¬1391æ¬¡: ç¬¬36å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 32.00, å¹³å‡å¥–åŠ±ä¸º 38.64
ç¬¬1524æ¬¡: ç¬¬37å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º133.00, å¹³å‡å¥–åŠ±ä¸º 41.19
ç¬¬1581æ¬¡: ç¬¬38å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 57.00, å¹³å‡å¥–åŠ±ä¸º 41.61
ç¬¬1764æ¬¡: ç¬¬39å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º183.00, å¹³å‡å¥–åŠ±ä¸º 45.23
ç¬¬1847æ¬¡: ç¬¬40å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 83.00, å¹³å‡å¥–åŠ±ä¸º 46.17
ç¬¬1898æ¬¡: ç¬¬41å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 51.00, å¹³å‡å¥–åŠ±ä¸º 46.29
ç¬¬1973æ¬¡: ç¬¬42å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 75.00, å¹³å‡å¥–åŠ±ä¸º 46.98
ç¬¬1996æ¬¡: ç¬¬43å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 23.00, å¹³å‡å¥–åŠ±ä¸º 46.42
ç¬¬2022æ¬¡: ç¬¬44å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 26.00, å¹³å‡å¥–åŠ±ä¸º 45.95
ç¬¬2092æ¬¡: ç¬¬45å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 70.00, å¹³å‡å¥–åŠ±ä¸º 46.49
ç¬¬2120æ¬¡: ç¬¬46å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 28.00, å¹³å‡å¥–åŠ±ä¸º 46.09
ç¬¬2154æ¬¡: ç¬¬47å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 34.00, å¹³å‡å¥–åŠ±ä¸º 45.83
ç¬¬2257æ¬¡: ç¬¬48å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º103.00, å¹³å‡å¥–åŠ±ä¸º 47.02
ç¬¬2334æ¬¡: ç¬¬49å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 77.00, å¹³å‡å¥–åŠ±ä¸º 47.63
ç¬¬2439æ¬¡: ç¬¬50å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º105.00, å¹³å‡å¥–åŠ±ä¸º 48.78
ç¬¬2508æ¬¡: ç¬¬51å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 69.00, å¹³å‡å¥–åŠ±ä¸º 49.18
ç¬¬2543æ¬¡: ç¬¬52å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 35.00, å¹³å‡å¥–åŠ±ä¸º 48.90
ç¬¬2630æ¬¡: ç¬¬53å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 87.00, å¹³å‡å¥–åŠ±ä¸º 49.62
ç¬¬2685æ¬¡: ç¬¬54å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 55.00, å¹³å‡å¥–åŠ±ä¸º 49.72
ç¬¬2705æ¬¡: ç¬¬55å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 20.00, å¹³å‡å¥–åŠ±ä¸º 49.18
ç¬¬2752æ¬¡: ç¬¬56å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 47.00, å¹³å‡å¥–åŠ±ä¸º 49.14
ç¬¬2862æ¬¡: ç¬¬57å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º110.00, å¹³å‡å¥–åŠ±ä¸º 50.21
ç»è¿‡2862è½®å®Œæˆ57å±€æ¸¸æˆ!</span></pre>
</div>
<h2><span style="font-family: 'comic sans ms', sans-serif;">4.&nbsp;04_cartpole_pg.py</span></h2>
<h3><span style="font-family: 'comic sans ms', sans-serif;">4.1 ç¨‹åº</span></h3>
<div class="cnblogs_Highlighter">
<pre class="brush:python;gutter:true;"><span style="font-family: 'comic sans ms', sans-serif;">#!/usr/bin/env python3
# -*- coding=utf-8 -*-
# Policy gradient methods on CartPole
# ä¸ºå¢åŠ æ¢ç´¢ï¼Œå¼•å…¥entropy bonus(ä¿¡æ¯ç†µæ­£åˆ™é¡¹)
# ä¸ºäº†é¼“åŠ±æ¨¡å‹åŠ å…¥æ›´å¤šçš„ä¸ç¡®å®šæ€§ï¼Œè¿™æ ·åœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œæ¨¡å‹å°±ä¼šå»æ¢ç´¢æ›´å¤šçš„å¯èƒ½æ€§
# H(&pi;) = - sum(&pi;(a|s) * log &pi;(a|s))
# min -(the discounted reward - baseline) * log &pi;(s, a) + beta * sum (&pi;(s, a) * log &pi;(s, a))
# https://www.cnblogs.com/kailugaji/
import gym
import ptan
import numpy as np
from tensorboardX import SummaryWriter
from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

GAMMA = 0.99 # æŠ˜æ‰£ç‡
LEARNING_RATE = 0.01 # å­¦ä¹ ç‡
ENTROPY_BETA = 0.01 # ç†µæ­£åˆ™åŒ–å› å­ the scale of the entropy bonus
BATCH_SIZE = 16 # ä¸€æ‰¹xxä¸ªæ ·æœ¬
REWARD_STEPS = 10
# ç”¨äºè¯´æ˜ä¸€æ¡è®°å½•ä¸­åŒ…å«çš„æ­¥(step)æ•° (sub-trajectories of length 10)
# how many steps ahead the Bellman equation is unrolled to estimate the discounted total reward of every transition.

# æ„å»ºç½‘ç»œ
class PGN(nn.Module):
    def __init__(self, input_size, n_actions):
        # input_sizeï¼šè¾“å…¥çŠ¶æ€ç»´åº¦ï¼Œhidden_sizeï¼šéšå±‚ç»´åº¦=128ï¼Œn_actionsï¼šè¾“å‡ºåŠ¨ä½œç»´åº¦
        super(PGN, self).__init__()

        self.net = nn.Sequential(
            nn.Linear(input_size, 128), # å…¨è¿æ¥å±‚ï¼Œéšå±‚é¢„è®¾ä¸º128ç»´åº¦
            nn.ReLU(),
            nn.Linear(128, n_actions) # å…¨è¿æ¥å±‚
        )

    def forward(self, x):
        return self.net(x) # æœªç”¨softmax

# å¹³æ»‘
def smooth(old: Optional[float], val: float, alpha: float = 0.95) -&gt; float:
    if old is None:
        return val
    return old * alpha + (1-alpha)*val


if __name__ == "__main__":
    env = gym.make("CartPole-v0") # åˆ›å»ºæ¸¸æˆç¯å¢ƒ
    writer = SummaryWriter(comment="-cartpole-pg")

    net = PGN(env.observation_space.shape[0], env.action_space.n) # 4(çŠ¶æ€)-&gt;128-&gt;2(åŠ¨ä½œ)
    # print(net)

    agent = ptan.agent.PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor, apply_softmax=True)
    # PolicyAgentï¼šè¿ç»­
    # make a decision about actions for every observation (ä¾æ¦‚ç‡)
    # apply_softmax=Trueï¼šç½‘ç»œè¾“å‡ºå…ˆç»è¿‡softmaxè½¬åŒ–æˆæ¦‚ç‡ï¼Œå†ä»è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸­è¿›è¡ŒéšæœºæŠ½æ ·
    # float32_preprocessorï¼šreturns the observation as float64 instead of the float32 required by PyTorch
    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA, steps_count=REWARD_STEPS)
    # è¿”å›è¿è¡Œè®°å½•ä»¥ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œè¾“å‡ºæ ¼å¼ä¸ºï¼š(state, action, reward, last_state)
    # å¹¶ä¸ä¼šè¾“å‡ºæ¯ä¸€æ­¥çš„ä¿¡æ¯ï¼Œè€Œæ˜¯æŠŠå¤šæ­¥çš„äº¤äº’ç»“æœç»¼åˆ(ç´¯è®¡å¤šæ­¥çš„reward;æ˜¾ç¤ºå¤´å°¾çš„çŠ¶æ€)åˆ°ä¸€æ¡Experienceè¾“å‡º
    # å¤šæ­¥rewardsçš„ç´¯åŠ æ˜¯æœ‰è¡°é€€çš„ï¼Œè€Œå…¶ä¸­çš„è¡°é€€ç³»æ•°ç”±å‚æ•°gamma(æŠ˜æ‰£ç‡)æŒ‡å®šï¼Œå³reward=r1+gamma&lowast;r2+(gamma^2)&lowast;r3
    # å…¶ä¸­rnä»£è¡¨ç¬¬næ­¥æ“ä½œè·å¾—çš„reward
    # last_state: the state we've got after executing the action. If our episode ends, we have None here
    # steps_count=REWARD_STEPSï¼šunroll the Bellman equation for 10 steps
    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE) # Adamä¼˜åŒ–

    total_rewards = [] # å‰å‡ å±€çš„å¥–åŠ±ä¹‹å’Œ
    step_rewards = []
    step_idx = 0 # è¿­ä»£æ¬¡æ•°/è½®æ•°
    done_episodes = 0 # å±€æ•°ï¼Œå‡ å±€æ¸¸æˆ
    reward_sum = 0.0 # the sum of the discounted reward for every transition
    bs_smoothed = entropy = l_entropy = l_policy = l_total = None

    batch_states, batch_actions, batch_scales = [], [], []

    for step_idx, exp in enumerate(exp_source): # (state, action, reward, last_state)
        reward_sum += exp.reward # the sum of the discounted reward for every transition
        baseline = reward_sum / (step_idx + 1) # å¥–åŠ±é™¤ä»¥è¿­ä»£æ¬¡æ•°(æ­¥æ•°) the baseline for the policy scale
        writer.add_scalar("baseline", baseline, step_idx)
        batch_states.append(exp.state) # çŠ¶æ€
        batch_actions.append(int(exp.action)) # åŠ¨ä½œ
        batch_scales.append(exp.reward - baseline) # ä¼˜åŠ¿å‡½æ•°ï¼Œå¼•å…¥åŸºå‡†

        # handle new rewards
        new_rewards = exp_source.pop_total_rewards() # è¿”å›ä¸€å±€æ¸¸æˆè¿‡åçš„total_rewords
        if new_rewards:
            done_episodes += 1 # æ¸¸æˆå±€æ•°(å›åˆæ•°)
            reward = new_rewards[0] # æœ¬å±€æ¸¸æˆå¥–åŠ±
            total_rewards.append(reward) # å‰å‡ å±€çš„å¥–åŠ±ä¹‹å’Œ
            mean_rewards = float(np.mean(total_rewards[-100:])) # å‰å‡ å±€çš„å¥–åŠ±ä¹‹å’Œ/å›åˆæ•°
            print("ç¬¬%dæ¬¡: ç¬¬%då±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º%6.2f, å¹³å‡å¥–åŠ±ä¸º%6.2f" % (step_idx, done_episodes, reward, mean_rewards))
            writer.add_scalar("reward", reward, step_idx) # æœ¬å±€æ¸¸æˆå¥–åŠ±
            writer.add_scalar("reward_100", mean_rewards, step_idx) # å‰å‡ å±€æ¸¸æˆçš„å¹³å‡å¥–åŠ±
            writer.add_scalar("episodes", done_episodes, step_idx) # æ¸¸æˆå±€æ•°(å›åˆæ•°)
            if mean_rewards &gt; 50: # æœ€å¤§æœŸæœ›å¥–åŠ±é˜ˆå€¼ï¼Œåªæœ‰å½“mean_rewards &gt; 50æ—¶æ‰ç»“æŸæ¸¸æˆ
                print("ç»è¿‡%dè½®å®Œæˆ%då±€æ¸¸æˆ!" % (step_idx, done_episodes))
                break

        if len(batch_states) &lt; BATCH_SIZE: # stateé‡Œé¢è¿˜æ²¡è¶…è¿‡BATCH_SIZEä¸ªæ ·æœ¬
            continue

        states_v = torch.FloatTensor(batch_states)
        batch_actions_t = torch.LongTensor(batch_actions)
        batch_scale_v = torch.FloatTensor(batch_scales) # ä¼˜åŠ¿å‡½æ•°ï¼Œå¼•å…¥åŸºå‡†

        # train
        optimizer.zero_grad()
        logits_v = net(states_v) # è¾“å…¥çŠ¶æ€ï¼Œè¾“å‡ºQ(s, a)å€¼
        log_prob_v = F.log_softmax(logits_v, dim=1) # è¾“å‡ºåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡åˆ†å¸ƒ log &pi;(s, a)
        log_prob_actions_v = batch_scale_v * log_prob_v[range(BATCH_SIZE), batch_actions_t]
        # max (the discounted reward - baseline) * log &pi;(s, a)
        loss_policy_v = -log_prob_actions_v.mean() # min

        # add the entropy bonus to the loss
        prob_v = F.softmax(logits_v, dim=1) # è¾“å‡ºåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ&pi;(s, a)
        entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean() # ç†µæ­£åˆ™é¡¹
        # ä¿¡æ¯ç†µçš„è®¡ç®—å…¬å¼ï¼šentropy = -sum (&pi;(s, a) * log &pi;(s, a))
        entropy_loss_v = -ENTROPY_BETA * entropy_v # loss = - beta * entropy
        loss_v = loss_policy_v + entropy_loss_v
        # min -(the discounted reward - baseline) * log &pi;(s, a) + beta * sum (&pi;(s, a) * log &pi;(s, a))

        loss_v.backward()
        optimizer.step()

        # calculate the Kullback-Leibler (KL) divergence between the new policy and the old policy
        new_logits_v = net(states_v) # è¾“å…¥çŠ¶æ€ï¼Œè¾“å‡ºQ(s, a)å€¼
        new_prob_v = F.softmax(new_logits_v, dim=1) # è¾“å‡ºåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ&pi;(s, a)
        kl_div_v = -((new_prob_v / prob_v).log() * prob_v).sum(dim=1).mean()
        # KLæ•£åº¦çš„è®¡ç®—å…¬å¼ï¼šKL = - sum(log(&pi;'(s, a)/&pi;(s, a)) * &pi;(s, a))
        writer.add_scalar("kl", kl_div_v.item(), step_idx)

        # calculate the statistics about the gradients on this training step
        grad_max = 0.0
        grad_means = 0.0
        grad_count = 0
        for p in net.parameters():
            grad_max = max(grad_max, p.grad.abs().max().item()) # the graph of the maximum
            grad_means += (p.grad ** 2).mean().sqrt().item() # L2 norm of gradients
            grad_count += 1

        # ä¸‹é¢å®é™…ä¸Šæ²¡æœ‰å¹³æ»‘ï¼Œè¿”å›å®é™…å€¼(è¿™5è¡Œä»£ç åœ¨æœ¬ç¨‹åºä¸­æ²¡æ„ä¹‰)
        bs_smoothed = smooth(bs_smoothed, np.mean(batch_scales))
        entropy = smooth(entropy, entropy_v.item())
        l_entropy = smooth(l_entropy, entropy_loss_v.item())
        l_policy = smooth(l_policy, loss_policy_v.item())
        l_total = smooth(l_total, loss_v.item())

        writer.add_scalar("baseline", baseline, step_idx)
        writer.add_scalar("entropy", entropy, step_idx)
        writer.add_scalar("loss_entropy", l_entropy, step_idx)
        writer.add_scalar("loss_policy", l_policy, step_idx)
        writer.add_scalar("loss_total", l_total, step_idx)
        writer.add_scalar("grad_l2", grad_means / grad_count, step_idx)
        writer.add_scalar("grad_max", grad_max, step_idx)
        writer.add_scalar("batch_scales", bs_smoothed, step_idx)

        batch_states.clear()
        batch_actions.clear()
        batch_scales.clear()

    writer.close()</span></pre>
</div>
<h3><span style="font-family: 'comic sans ms', sans-serif;">4.2 ç»“æœ</span></h3>
<div class="cnblogs_Highlighter">
<pre class="brush:python;gutter:true;"><span style="font-family: 'comic sans ms', sans-serif;">ç¬¬16æ¬¡: ç¬¬1å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 16.00, å¹³å‡å¥–åŠ±ä¸º 16.00
ç¬¬35æ¬¡: ç¬¬2å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 19.00, å¹³å‡å¥–åŠ±ä¸º 17.50
ç¬¬72æ¬¡: ç¬¬3å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 37.00, å¹³å‡å¥–åŠ±ä¸º 24.00
ç¬¬90æ¬¡: ç¬¬4å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 18.00, å¹³å‡å¥–åŠ±ä¸º 22.50
ç¬¬100æ¬¡: ç¬¬5å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 10.00, å¹³å‡å¥–åŠ±ä¸º 20.00
ç¬¬121æ¬¡: ç¬¬6å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 21.00, å¹³å‡å¥–åŠ±ä¸º 20.17
ç¬¬130æ¬¡: ç¬¬7å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º  9.00, å¹³å‡å¥–åŠ±ä¸º 18.57
ç¬¬143æ¬¡: ç¬¬8å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 13.00, å¹³å‡å¥–åŠ±ä¸º 17.88
ç¬¬156æ¬¡: ç¬¬9å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 13.00, å¹³å‡å¥–åŠ±ä¸º 17.33
ç¬¬170æ¬¡: ç¬¬10å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 14.00, å¹³å‡å¥–åŠ±ä¸º 17.00
ç¬¬185æ¬¡: ç¬¬11å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 15.00, å¹³å‡å¥–åŠ±ä¸º 16.82
ç¬¬198æ¬¡: ç¬¬12å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 13.00, å¹³å‡å¥–åŠ±ä¸º 16.50
ç¬¬224æ¬¡: ç¬¬13å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 26.00, å¹³å‡å¥–åŠ±ä¸º 17.23
ç¬¬253æ¬¡: ç¬¬14å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 29.00, å¹³å‡å¥–åŠ±ä¸º 18.07
ç¬¬323æ¬¡: ç¬¬15å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 70.00, å¹³å‡å¥–åŠ±ä¸º 21.53
ç¬¬356æ¬¡: ç¬¬16å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 33.00, å¹³å‡å¥–åŠ±ä¸º 22.25
ç¬¬388æ¬¡: ç¬¬17å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 32.00, å¹³å‡å¥–åŠ±ä¸º 22.82
ç¬¬414æ¬¡: ç¬¬18å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 26.00, å¹³å‡å¥–åŠ±ä¸º 23.00
ç¬¬442æ¬¡: ç¬¬19å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 28.00, å¹³å‡å¥–åŠ±ä¸º 23.26
ç¬¬536æ¬¡: ç¬¬20å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 94.00, å¹³å‡å¥–åŠ±ä¸º 26.80
ç¬¬589æ¬¡: ç¬¬21å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 53.00, å¹³å‡å¥–åŠ±ä¸º 28.05
ç¬¬652æ¬¡: ç¬¬22å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 63.00, å¹³å‡å¥–åŠ±ä¸º 29.64
ç¬¬753æ¬¡: ç¬¬23å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º101.00, å¹³å‡å¥–åŠ±ä¸º 32.74
ç¬¬860æ¬¡: ç¬¬24å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º107.00, å¹³å‡å¥–åŠ±ä¸º 35.83
ç¬¬904æ¬¡: ç¬¬25å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 44.00, å¹³å‡å¥–åŠ±ä¸º 36.16
ç¬¬984æ¬¡: ç¬¬26å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 80.00, å¹³å‡å¥–åŠ±ä¸º 37.85
ç¬¬1174æ¬¡: ç¬¬27å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º190.00, å¹³å‡å¥–åŠ±ä¸º 43.48
ç¬¬1236æ¬¡: ç¬¬28å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 62.00, å¹³å‡å¥–åŠ±ä¸º 44.14
ç¬¬1317æ¬¡: ç¬¬29å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 81.00, å¹³å‡å¥–åŠ±ä¸º 45.41
ç¬¬1381æ¬¡: ç¬¬30å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 64.00, å¹³å‡å¥–åŠ±ä¸º 46.03
ç¬¬1581æ¬¡: ç¬¬31å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º200.00, å¹³å‡å¥–åŠ±ä¸º 51.00
ç»è¿‡1581è½®å®Œæˆ31å±€æ¸¸æˆ!
</span></pre>
</div>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202202/1027447-20220228161059841-1707305522.png" alt="" width="708" height="288" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">The baseline value during the training</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202202/1027447-20220228161140254-1911541991.png" alt="" width="711" height="284" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Batch scales</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202202/1027447-20220228161238767-1277988018.png" alt="" width="732" height="287" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Entropy during the training</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202202/1027447-20220228161341071-1849887479.png" alt="" width="738" height="292" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Entropy loss</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202202/1027447-20220228161417197-64783980.png" alt="" width="742" height="297" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Policy loss</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202202/1027447-20220228161458001-1061025591.png" alt="" width="751" height="302" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Total loss</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">å…¶ä»–ç»“æœï¼Œè¯·çœ‹ï¼š<a href="https://github.com/kailugaji/Hands-on-Reinforcement-Learning/tree/main/03%20Policy%20Gradients/results" target="_blank">https://github.com/kailugaji/Hands-on-Reinforcement-Learning/tree/main/03%20Policy%20Gradients/results</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">ä»Lossç»“æœçœ‹ï¼Œè¿™æ¬¡å®éªŒå¹¶ä¸ç†æƒ³ï¼Œå¯ä»¥å¤šè¿è¡Œå‡ æ¬¡ï¼Œå¾—åˆ°æ›´å¥½çš„ç»“æœã€‚</span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">5. å‰å››ä¸ªç®—æ³•çš„æ€»ä½“ç»“æœå¯¹æ¯”</span></h2>
<p style="text-align: left;"><span style="color: #33cccc; font-family: 'comic sans ms', sans-serif; font-size: 16px;">è“çº¿ï¼šDQN(01_cartpole_dqn.py)</span></p>
<p style="text-align: left;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">é»‘çº¿ï¼šREINFORCE(02_cartpole_reinforce.py)</span></p>
<p style="text-align: left;"><span style="color: #ff00ff; font-family: 'comic sans ms', sans-serif; font-size: 16px;">çº¢çº¿ï¼šREINFORCE with Baseline(03_cartpole_reinforce_baseline.py)</span></p>
<p style="text-align: left;"><span style="color: #ffcc00; font-family: 'comic sans ms', sans-serif; font-size: 16px;">é»„çº¿ï¼šç­–ç•¥æ¢¯åº¦æ³•(04_cartpole_pg.py)</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202202/1027447-20220228161932627-643908632.png" alt="" width="770" height="309" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Reward</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202202/1027447-20220228162105218-1477301521.png" alt="" width="756" height="302" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Average reward</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2022.cnblogs.com/blog/1027447/202202/1027447-20220228162153915-1781774841.png" alt="" width="764" height="305" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Episodes</span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">6.&nbsp;06_cartpole_pg.py</span></h2>
<h3><span style="font-family: 'comic sans ms', sans-serif;">6.1 ç¨‹åº</span></h3>
<div class="cnblogs_Highlighter">
<pre class="brush:python;gutter:true;">#!/usr/bin/env python3
# -*- coding=utf-8 -*-
# Policy gradient methods on CartPole
# ä¸ºå¢åŠ æ¢ç´¢ï¼Œå¼•å…¥entropy bonus(ä¿¡æ¯ç†µæ­£åˆ™é¡¹)
# ä¸ºäº†é¼“åŠ±æ¨¡å‹åŠ å…¥æ›´å¤šçš„ä¸ç¡®å®šæ€§ï¼Œè¿™æ ·åœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œæ¨¡å‹å°±ä¼šå»æ¢ç´¢æ›´å¤šçš„å¯èƒ½æ€§
# H(&pi;) = - sum(&pi;(a|s) * log &pi;(a|s))
# ä¸03 Policy Gradients/04_cartpole_pg.pyåŸºæœ¬ä¸€è‡´ï¼Œå”¯ä¸€ä¸åŒæ˜¯Baselineå¯é€‰æ‹©ï¼ŒåŠ æˆ–è€…ä¸åŠ 
# åŠ ï¼šmin -(the discounted reward - baseline) * log &pi;(s, a) + beta * sum (&pi;(s, a) * log &pi;(s, a))
# ä¸åŠ ï¼šmin -(the discounted reward) * log &pi;(s, a) + beta * sum (&pi;(s, a) * log &pi;(s, a))
# https://www.cnblogs.com/kailugaji/
import gym
import ptan
import argparse
import numpy as np
from tensorboardX import SummaryWriter

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

GAMMA = 0.99 # æŠ˜æ‰£ç‡
LEARNING_RATE = 0.01 # å­¦ä¹ ç‡
ENTROPY_BETA = 0.01 # ç†µæ­£åˆ™åŒ–å› å­ the scale of the entropy bonus
BATCH_SIZE = 16 # ä¸€æ‰¹xxä¸ªæ ·æœ¬

REWARD_STEPS = 10
# ç”¨äºè¯´æ˜ä¸€æ¡è®°å½•ä¸­åŒ…å«çš„æ­¥(step)æ•° (sub-trajectories of length 10)
# how many steps ahead the Bellman equation is unrolled to estimate the discounted total reward of every transition.

# æ„å»ºç½‘ç»œ
class PGN(nn.Module):
    def __init__(self, input_size, n_actions):
        # input_sizeï¼šè¾“å…¥çŠ¶æ€ç»´åº¦ï¼Œhidden_sizeï¼šéšå±‚ç»´åº¦=128ï¼Œn_actionsï¼šè¾“å‡ºåŠ¨ä½œç»´åº¦
        super(PGN, self).__init__()

        self.net = nn.Sequential(
            nn.Linear(input_size, 128), # å…¨è¿æ¥å±‚ï¼Œéšå±‚é¢„è®¾ä¸º128ç»´åº¦
            nn.ReLU(),
            nn.Linear(128, n_actions) # å…¨è¿æ¥å±‚
        )

    def forward(self, x):
        return self.net(x) # æœªç”¨softmax


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--baseline", default=False, action='store_true', help="Enable mean baseline")
    args = parser.parse_args()
    args.baseline = "True" # åŠ baseline

    env = gym.make("CartPole-v0") # åˆ›å»ºæ¸¸æˆç¯å¢ƒ
    writer = SummaryWriter(comment="-cartpole-pg" + "-baseline=%s" % args.baseline)

    net = PGN(env.observation_space.shape[0], env.action_space.n) # 4(çŠ¶æ€)-&gt;128-&gt;2(åŠ¨ä½œ)
    # print(net)

    agent = ptan.agent.PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor, apply_softmax=True)
    # PolicyAgentï¼šè¿ç»­
    # make a decision about actions for every observation (ä¾æ¦‚ç‡)
    # apply_softmax=Trueï¼šç½‘ç»œè¾“å‡ºå…ˆç»è¿‡softmaxè½¬åŒ–æˆæ¦‚ç‡ï¼Œå†ä»è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸­è¿›è¡ŒéšæœºæŠ½æ ·
    # float32_preprocessorï¼šreturns the observation as float64 instead of the float32 required by PyTorch
    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA, steps_count=REWARD_STEPS)
    # è¿”å›è¿è¡Œè®°å½•ä»¥ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œè¾“å‡ºæ ¼å¼ä¸ºï¼š(state, action, reward, last_state)
    # å¹¶ä¸ä¼šè¾“å‡ºæ¯ä¸€æ­¥çš„ä¿¡æ¯ï¼Œè€Œæ˜¯æŠŠå¤šæ­¥çš„äº¤äº’ç»“æœç»¼åˆ(ç´¯è®¡å¤šæ­¥çš„reward;æ˜¾ç¤ºå¤´å°¾çš„çŠ¶æ€)åˆ°ä¸€æ¡Experienceè¾“å‡º
    # å¤šæ­¥rewardsçš„ç´¯åŠ æ˜¯æœ‰è¡°é€€çš„ï¼Œè€Œå…¶ä¸­çš„è¡°é€€ç³»æ•°ç”±å‚æ•°gamma(æŠ˜æ‰£ç‡)æŒ‡å®šï¼Œå³reward=r1+gamma&lowast;r2+(gamma^2)&lowast;r3
    # å…¶ä¸­rnä»£è¡¨ç¬¬næ­¥æ“ä½œè·å¾—çš„reward
    # last_state: the state we've got after executing the action. If our episode ends, we have None here
    # steps_count=REWARD_STEPSï¼šunroll the Bellman equation for 10 steps
    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE) # Adamä¼˜åŒ–

    total_rewards = [] # å‰å‡ å±€çš„å¥–åŠ±ä¹‹å’Œ
    step_rewards = []
    step_idx = 0 # è¿­ä»£æ¬¡æ•°/è½®æ•°
    done_episodes = 0 # å±€æ•°ï¼Œå‡ å±€æ¸¸æˆ
    reward_sum = 0.0 # the sum of the discounted reward for every transition

    batch_states, batch_actions, batch_scales = [], [], []

    for step_idx, exp in enumerate(exp_source): # (state, action, reward, last_state)
        reward_sum += exp.reward # the sum of the discounted reward for every transition
        baseline = reward_sum / (step_idx + 1) # å¥–åŠ±é™¤ä»¥è¿­ä»£æ¬¡æ•°(æ­¥æ•°) the baseline for the policy scale
        writer.add_scalar("baseline", baseline, step_idx)
        batch_states.append(exp.state) # çŠ¶æ€
        batch_actions.append(int(exp.action)) # åŠ¨ä½œ
        if args.baseline: # True
            batch_scales.append(exp.reward - baseline) # ä¼˜åŠ¿å‡½æ•°ï¼Œå¼•å…¥åŸºå‡†
        else: # False
            batch_scales.append(exp.reward) # æ²¡æœ‰åŸºå‡†

        # handle new rewards
        new_rewards = exp_source.pop_total_rewards() # è¿”å›ä¸€å±€æ¸¸æˆè¿‡åçš„total_rewords
        if new_rewards:
            done_episodes += 1 # æ¸¸æˆå±€æ•°(å›åˆæ•°)
            reward = new_rewards[0] # æœ¬å±€æ¸¸æˆå¥–åŠ±
            total_rewards.append(reward) # å‰å‡ å±€çš„å¥–åŠ±ä¹‹å’Œ
            mean_rewards = float(np.mean(total_rewards[-100:])) # å¹³å‡å¥–åŠ±ï¼šå‰å‡ å±€çš„å¥–åŠ±ä¹‹å’Œ/å›åˆæ•°
            print("ç¬¬%dæ¬¡: ç¬¬%då±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º%6.2f, å¹³å‡å¥–åŠ±ä¸º%6.2f" % (step_idx, done_episodes, reward, mean_rewards))
            writer.add_scalar("reward", reward, step_idx) # æœ¬å±€æ¸¸æˆå¥–åŠ±
            writer.add_scalar("reward_100", mean_rewards, step_idx) # å‰å‡ å±€æ¸¸æˆçš„å¹³å‡å¥–åŠ±
            writer.add_scalar("episodes", done_episodes, step_idx) # æ¸¸æˆå±€æ•°(å›åˆæ•°)
            if mean_rewards &gt; 50: # æœ€å¤§æœŸæœ›å¥–åŠ±é˜ˆå€¼ï¼Œåªæœ‰å½“mean_rewards &gt; 50æ—¶æ‰ç»“æŸæ¸¸æˆ
                print("ç»è¿‡%dè½®å®Œæˆ%då±€æ¸¸æˆ!" % (step_idx, done_episodes))
                break

        if len(batch_states) &lt; BATCH_SIZE: # stateé‡Œé¢è¿˜æ²¡è¶…è¿‡BATCH_SIZEä¸ªæ ·æœ¬
            continue

        states_v = torch.FloatTensor(batch_states)
        batch_actions_t = torch.LongTensor(batch_actions)
        batch_scale_v = torch.FloatTensor(batch_scales) # r æˆ–è€… ä¼˜åŠ¿å‡½æ•°(r - b)

        # train
        optimizer.zero_grad()
        logits_v = net(states_v) # è¾“å…¥çŠ¶æ€ï¼Œè¾“å‡ºQ(s, a)å€¼
        log_prob_v = F.log_softmax(logits_v, dim=1) # è¾“å‡ºåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡åˆ†å¸ƒ log &pi;(s, a)
        log_prob_actions_v = batch_scale_v * log_prob_v[range(BATCH_SIZE), batch_actions_t]
        # åŠ åŸºå‡†ï¼šmax (the discounted reward - baseline) * log &pi;(s, a)
        # ä¸åŠ åŸºå‡†ï¼šmax (the discounted reward) * log &pi;(s, a)
        loss_policy_v = -log_prob_actions_v.mean() # min

        loss_policy_v.backward(retain_graph=True)
        grads = np.concatenate([p.grad.data.numpy().flatten()
                                for p in net.parameters()
                                if p.grad is not None])

        # add the entropy bonus to the loss
        prob_v = F.softmax(logits_v, dim=1) # è¾“å‡ºåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ&pi;(s, a)
        entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean() # ç†µæ­£åˆ™é¡¹
        # ä¿¡æ¯ç†µçš„è®¡ç®—å…¬å¼ï¼šentropy = -sum (&pi;(s, a) * log &pi;(s, a))
        entropy_loss_v = -ENTROPY_BETA * entropy_v # loss = - beta * entropy
        entropy_loss_v.backward()
        optimizer.step()

        loss_v = loss_policy_v + entropy_loss_v
        # åŠ åŸºå‡†ï¼šmin -(the discounted reward - baseline) * log &pi;(s, a) + beta * sum (&pi;(s, a) * log &pi;(s, a))
        # ä¸åŠ ï¼šmin -(the discounted reward) * log &pi;(s, a) + beta * sum (&pi;(s, a) * log &pi;(s, a))

        # calculate the Kullback-Leibler (KL) divergence between the new policy and the old policy
        new_logits_v = net(states_v) # è¾“å…¥çŠ¶æ€ï¼Œè¾“å‡ºQ(s, a)å€¼
        new_prob_v = F.softmax(new_logits_v, dim=1) # è¾“å‡ºåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ&pi;(s, a)
        kl_div_v = -((new_prob_v / prob_v).log() * prob_v).sum(dim=1).mean()
        # KLæ•£åº¦çš„è®¡ç®—å…¬å¼ï¼šKL = - sum(log(&pi;'(s, a)/&pi;(s, a)) * &pi;(s, a))
        writer.add_scalar("kl", kl_div_v.item(), step_idx)

        writer.add_scalar("baseline", baseline, step_idx)
        writer.add_scalar("entropy", entropy_v.item(), step_idx)
        writer.add_scalar("batch_scales", np.mean(batch_scales), step_idx)
        writer.add_scalar("loss_entropy", entropy_loss_v.item(), step_idx)
        writer.add_scalar("loss_policy", loss_policy_v.item(), step_idx)
        writer.add_scalar("loss_total", loss_v.item(), step_idx)

        # calculate the statistics about the gradients on this training step
        g_l2 = np.sqrt(np.mean(np.square(grads))) # L2 norm of gradients
        g_max = np.max(np.abs(grads)) # the graph of the maximum
        writer.add_scalar("grad_l2", g_l2, step_idx)
        writer.add_scalar("grad_max", g_max, step_idx)
        writer.add_scalar("grad_var", np.var(grads), step_idx) # æ¢¯åº¦æ–¹å·®

        batch_states.clear()
        batch_actions.clear()
        batch_scales.clear()

    writer.close()</pre>
</div>
<h3><span style="font-family: 'comic sans ms', sans-serif;">6.2 ç»“æœ</span></h3>
<p><span style="font-size: 16px;"><span style="font-family: 'comic sans ms', sans-serif;">å½“ä¸åŠ baselineæ—¶ï¼š</span></span></p>
<div class="cnblogs_Highlighter">
<pre class="brush:python;gutter:true;">ç¬¬32æ¬¡: ç¬¬1å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 32.00, å¹³å‡å¥–åŠ±ä¸º 32.00
ç¬¬47æ¬¡: ç¬¬2å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 15.00, å¹³å‡å¥–åŠ±ä¸º 23.50
ç¬¬65æ¬¡: ç¬¬3å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 18.00, å¹³å‡å¥–åŠ±ä¸º 21.67
ç¬¬74æ¬¡: ç¬¬4å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º  9.00, å¹³å‡å¥–åŠ±ä¸º 18.50
ç¬¬87æ¬¡: ç¬¬5å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 13.00, å¹³å‡å¥–åŠ±ä¸º 17.40
ç¬¬101æ¬¡: ç¬¬6å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 14.00, å¹³å‡å¥–åŠ±ä¸º 16.83
ç¬¬113æ¬¡: ç¬¬7å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 12.00, å¹³å‡å¥–åŠ±ä¸º 16.14
ç¬¬124æ¬¡: ç¬¬8å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 11.00, å¹³å‡å¥–åŠ±ä¸º 15.50
ç¬¬138æ¬¡: ç¬¬9å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 14.00, å¹³å‡å¥–åŠ±ä¸º 15.33
ç¬¬148æ¬¡: ç¬¬10å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 10.00, å¹³å‡å¥–åŠ±ä¸º 14.80
ç¬¬159æ¬¡: ç¬¬11å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 11.00, å¹³å‡å¥–åŠ±ä¸º 14.45
ç¬¬170æ¬¡: ç¬¬12å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 11.00, å¹³å‡å¥–åŠ±ä¸º 14.17
ç¬¬181æ¬¡: ç¬¬13å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 11.00, å¹³å‡å¥–åŠ±ä¸º 13.92
ç¬¬200æ¬¡: ç¬¬14å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 19.00, å¹³å‡å¥–åŠ±ä¸º 14.29
ç¬¬210æ¬¡: ç¬¬15å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 10.00, å¹³å‡å¥–åŠ±ä¸º 14.00
ç¬¬225æ¬¡: ç¬¬16å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 15.00, å¹³å‡å¥–åŠ±ä¸º 14.06
ç¬¬238æ¬¡: ç¬¬17å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 13.00, å¹³å‡å¥–åŠ±ä¸º 14.00
ç¬¬250æ¬¡: ç¬¬18å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 12.00, å¹³å‡å¥–åŠ±ä¸º 13.89
ç¬¬262æ¬¡: ç¬¬19å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 12.00, å¹³å‡å¥–åŠ±ä¸º 13.79
ç¬¬275æ¬¡: ç¬¬20å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 13.00, å¹³å‡å¥–åŠ±ä¸º 13.75
ç¬¬288æ¬¡: ç¬¬21å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 13.00, å¹³å‡å¥–åŠ±ä¸º 13.71
ç¬¬302æ¬¡: ç¬¬22å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 14.00, å¹³å‡å¥–åŠ±ä¸º 13.73
ç¬¬323æ¬¡: ç¬¬23å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 21.00, å¹³å‡å¥–åŠ±ä¸º 14.04
ç¬¬339æ¬¡: ç¬¬24å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 16.00, å¹³å‡å¥–åŠ±ä¸º 14.12
ç¬¬358æ¬¡: ç¬¬25å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 19.00, å¹³å‡å¥–åŠ±ä¸º 14.32
ç¬¬369æ¬¡: ç¬¬26å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 11.00, å¹³å‡å¥–åŠ±ä¸º 14.19
ç¬¬384æ¬¡: ç¬¬27å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 15.00, å¹³å‡å¥–åŠ±ä¸º 14.22
ç¬¬404æ¬¡: ç¬¬28å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 20.00, å¹³å‡å¥–åŠ±ä¸º 14.43
ç¬¬428æ¬¡: ç¬¬29å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 24.00, å¹³å‡å¥–åŠ±ä¸º 14.76
ç¬¬444æ¬¡: ç¬¬30å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 16.00, å¹³å‡å¥–åŠ±ä¸º 14.80
ç¬¬457æ¬¡: ç¬¬31å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 13.00, å¹³å‡å¥–åŠ±ä¸º 14.74
ç¬¬475æ¬¡: ç¬¬32å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 18.00, å¹³å‡å¥–åŠ±ä¸º 14.84
ç¬¬497æ¬¡: ç¬¬33å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 22.00, å¹³å‡å¥–åŠ±ä¸º 15.06
ç¬¬511æ¬¡: ç¬¬34å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 14.00, å¹³å‡å¥–åŠ±ä¸º 15.03
ç¬¬534æ¬¡: ç¬¬35å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 23.00, å¹³å‡å¥–åŠ±ä¸º 15.26
ç¬¬579æ¬¡: ç¬¬36å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 45.00, å¹³å‡å¥–åŠ±ä¸º 16.08
ç¬¬600æ¬¡: ç¬¬37å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 21.00, å¹³å‡å¥–åŠ±ä¸º 16.22
ç¬¬623æ¬¡: ç¬¬38å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 23.00, å¹³å‡å¥–åŠ±ä¸º 16.39
ç¬¬650æ¬¡: ç¬¬39å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 27.00, å¹³å‡å¥–åŠ±ä¸º 16.67
ç¬¬667æ¬¡: ç¬¬40å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 17.00, å¹³å‡å¥–åŠ±ä¸º 16.68
ç¬¬718æ¬¡: ç¬¬41å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 51.00, å¹³å‡å¥–åŠ±ä¸º 17.51
ç¬¬746æ¬¡: ç¬¬42å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 28.00, å¹³å‡å¥–åŠ±ä¸º 17.76
ç¬¬792æ¬¡: ç¬¬43å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 46.00, å¹³å‡å¥–åŠ±ä¸º 18.42
ç¬¬834æ¬¡: ç¬¬44å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 42.00, å¹³å‡å¥–åŠ±ä¸º 18.95
ç¬¬882æ¬¡: ç¬¬45å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 48.00, å¹³å‡å¥–åŠ±ä¸º 19.60
ç¬¬923æ¬¡: ç¬¬46å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 41.00, å¹³å‡å¥–åŠ±ä¸º 20.07
ç¬¬953æ¬¡: ç¬¬47å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 30.00, å¹³å‡å¥–åŠ±ä¸º 20.28
ç¬¬983æ¬¡: ç¬¬48å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 30.00, å¹³å‡å¥–åŠ±ä¸º 20.48
ç¬¬1071æ¬¡: ç¬¬49å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 88.00, å¹³å‡å¥–åŠ±ä¸º 21.86
ç¬¬1160æ¬¡: ç¬¬50å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 89.00, å¹³å‡å¥–åŠ±ä¸º 23.20
ç¬¬1287æ¬¡: ç¬¬51å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º127.00, å¹³å‡å¥–åŠ±ä¸º 25.24
ç¬¬1367æ¬¡: ç¬¬52å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 80.00, å¹³å‡å¥–åŠ±ä¸º 26.29
ç¬¬1469æ¬¡: ç¬¬53å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º102.00, å¹³å‡å¥–åŠ±ä¸º 27.72
ç¬¬1533æ¬¡: ç¬¬54å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 64.00, å¹³å‡å¥–åŠ±ä¸º 28.39
ç¬¬1589æ¬¡: ç¬¬55å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 56.00, å¹³å‡å¥–åŠ±ä¸º 28.89
ç¬¬1618æ¬¡: ç¬¬56å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 29.00, å¹³å‡å¥–åŠ±ä¸º 28.89
ç¬¬1651æ¬¡: ç¬¬57å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 33.00, å¹³å‡å¥–åŠ±ä¸º 28.96
ç¬¬1697æ¬¡: ç¬¬58å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 46.00, å¹³å‡å¥–åŠ±ä¸º 29.26
ç¬¬1707æ¬¡: ç¬¬59å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 10.00, å¹³å‡å¥–åŠ±ä¸º 28.93
ç¬¬1747æ¬¡: ç¬¬60å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 40.00, å¹³å‡å¥–åŠ±ä¸º 29.12
ç¬¬1778æ¬¡: ç¬¬61å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 31.00, å¹³å‡å¥–åŠ±ä¸º 29.15
ç¬¬1802æ¬¡: ç¬¬62å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 24.00, å¹³å‡å¥–åŠ±ä¸º 29.06
ç¬¬1820æ¬¡: ç¬¬63å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 18.00, å¹³å‡å¥–åŠ±ä¸º 28.89
ç¬¬1850æ¬¡: ç¬¬64å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 30.00, å¹³å‡å¥–åŠ±ä¸º 28.91
ç¬¬1862æ¬¡: ç¬¬65å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 12.00, å¹³å‡å¥–åŠ±ä¸º 28.65
ç¬¬1883æ¬¡: ç¬¬66å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 21.00, å¹³å‡å¥–åŠ±ä¸º 28.53
ç¬¬1919æ¬¡: ç¬¬67å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 36.00, å¹³å‡å¥–åŠ±ä¸º 28.64
ç¬¬1971æ¬¡: ç¬¬68å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 52.00, å¹³å‡å¥–åŠ±ä¸º 28.99
ç¬¬1996æ¬¡: ç¬¬69å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 25.00, å¹³å‡å¥–åŠ±ä¸º 28.93
ç¬¬2029æ¬¡: ç¬¬70å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 33.00, å¹³å‡å¥–åŠ±ä¸º 28.99
ç¬¬2104æ¬¡: ç¬¬71å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 75.00, å¹³å‡å¥–åŠ±ä¸º 29.63
ç¬¬2177æ¬¡: ç¬¬72å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 73.00, å¹³å‡å¥–åŠ±ä¸º 30.24
ç¬¬2270æ¬¡: ç¬¬73å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 93.00, å¹³å‡å¥–åŠ±ä¸º 31.10
ç¬¬2363æ¬¡: ç¬¬74å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 93.00, å¹³å‡å¥–åŠ±ä¸º 31.93
ç¬¬2530æ¬¡: ç¬¬75å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º167.00, å¹³å‡å¥–åŠ±ä¸º 33.73
ç¬¬2608æ¬¡: ç¬¬76å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 78.00, å¹³å‡å¥–åŠ±ä¸º 34.32
ç¬¬2703æ¬¡: ç¬¬77å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 95.00, å¹³å‡å¥–åŠ±ä¸º 35.10
ç¬¬2854æ¬¡: ç¬¬78å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º151.00, å¹³å‡å¥–åŠ±ä¸º 36.59
ç¬¬3054æ¬¡: ç¬¬79å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º200.00, å¹³å‡å¥–åŠ±ä¸º 38.66
ç¬¬3152æ¬¡: ç¬¬80å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 98.00, å¹³å‡å¥–åŠ±ä¸º 39.40
ç¬¬3205æ¬¡: ç¬¬81å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 53.00, å¹³å‡å¥–åŠ±ä¸º 39.57
ç¬¬3228æ¬¡: ç¬¬82å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 23.00, å¹³å‡å¥–åŠ±ä¸º 39.37
ç¬¬3258æ¬¡: ç¬¬83å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 30.00, å¹³å‡å¥–åŠ±ä¸º 39.25
ç¬¬3279æ¬¡: ç¬¬84å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 21.00, å¹³å‡å¥–åŠ±ä¸º 39.04
ç¬¬3301æ¬¡: ç¬¬85å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 22.00, å¹³å‡å¥–åŠ±ä¸º 38.84
ç¬¬3328æ¬¡: ç¬¬86å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 27.00, å¹³å‡å¥–åŠ±ä¸º 38.70
ç¬¬3346æ¬¡: ç¬¬87å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 18.00, å¹³å‡å¥–åŠ±ä¸º 38.46
ç¬¬3366æ¬¡: ç¬¬88å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 20.00, å¹³å‡å¥–åŠ±ä¸º 38.25
ç¬¬3384æ¬¡: ç¬¬89å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 18.00, å¹³å‡å¥–åŠ±ä¸º 38.02
ç¬¬3399æ¬¡: ç¬¬90å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 15.00, å¹³å‡å¥–åŠ±ä¸º 37.77
ç¬¬3420æ¬¡: ç¬¬91å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 21.00, å¹³å‡å¥–åŠ±ä¸º 37.58
ç¬¬3440æ¬¡: ç¬¬92å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 20.00, å¹³å‡å¥–åŠ±ä¸º 37.39
ç¬¬3466æ¬¡: ç¬¬93å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 26.00, å¹³å‡å¥–åŠ±ä¸º 37.27
ç¬¬3489æ¬¡: ç¬¬94å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 23.00, å¹³å‡å¥–åŠ±ä¸º 37.12
ç¬¬3506æ¬¡: ç¬¬95å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 17.00, å¹³å‡å¥–åŠ±ä¸º 36.91
ç¬¬3531æ¬¡: ç¬¬96å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 25.00, å¹³å‡å¥–åŠ±ä¸º 36.78
ç¬¬3563æ¬¡: ç¬¬97å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 32.00, å¹³å‡å¥–åŠ±ä¸º 36.73
ç¬¬3592æ¬¡: ç¬¬98å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 29.00, å¹³å‡å¥–åŠ±ä¸º 36.65
ç¬¬3617æ¬¡: ç¬¬99å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 25.00, å¹³å‡å¥–åŠ±ä¸º 36.54
ç¬¬3678æ¬¡: ç¬¬100å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 61.00, å¹³å‡å¥–åŠ±ä¸º 36.78
ç¬¬3734æ¬¡: ç¬¬101å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 56.00, å¹³å‡å¥–åŠ±ä¸º 37.02
ç¬¬3814æ¬¡: ç¬¬102å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 80.00, å¹³å‡å¥–åŠ±ä¸º 37.67
ç¬¬3987æ¬¡: ç¬¬103å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º173.00, å¹³å‡å¥–åŠ±ä¸º 39.22
ç¬¬4106æ¬¡: ç¬¬104å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º119.00, å¹³å‡å¥–åŠ±ä¸º 40.32
ç¬¬4153æ¬¡: ç¬¬105å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 47.00, å¹³å‡å¥–åŠ±ä¸º 40.66
ç¬¬4229æ¬¡: ç¬¬106å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 76.00, å¹³å‡å¥–åŠ±ä¸º 41.28
ç¬¬4365æ¬¡: ç¬¬107å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º136.00, å¹³å‡å¥–åŠ±ä¸º 42.52
ç¬¬4470æ¬¡: ç¬¬108å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º105.00, å¹³å‡å¥–åŠ±ä¸º 43.46
ç¬¬4532æ¬¡: ç¬¬109å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 62.00, å¹³å‡å¥–åŠ±ä¸º 43.94
ç¬¬4588æ¬¡: ç¬¬110å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 56.00, å¹³å‡å¥–åŠ±ä¸º 44.40
ç¬¬4669æ¬¡: ç¬¬111å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 81.00, å¹³å‡å¥–åŠ±ä¸º 45.10
ç¬¬4736æ¬¡: ç¬¬112å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 67.00, å¹³å‡å¥–åŠ±ä¸º 45.66
ç¬¬4797æ¬¡: ç¬¬113å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 61.00, å¹³å‡å¥–åŠ±ä¸º 46.16
ç¬¬4890æ¬¡: ç¬¬114å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 93.00, å¹³å‡å¥–åŠ±ä¸º 46.90
ç¬¬4980æ¬¡: ç¬¬115å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 90.00, å¹³å‡å¥–åŠ±ä¸º 47.70
ç¬¬5050æ¬¡: ç¬¬116å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 70.00, å¹³å‡å¥–åŠ±ä¸º 48.25
ç¬¬5089æ¬¡: ç¬¬117å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 39.00, å¹³å‡å¥–åŠ±ä¸º 48.51
ç¬¬5140æ¬¡: ç¬¬118å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 51.00, å¹³å‡å¥–åŠ±ä¸º 48.90
ç¬¬5176æ¬¡: ç¬¬119å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 36.00, å¹³å‡å¥–åŠ±ä¸º 49.14
ç¬¬5207æ¬¡: ç¬¬120å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 31.00, å¹³å‡å¥–åŠ±ä¸º 49.32
ç¬¬5240æ¬¡: ç¬¬121å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 33.00, å¹³å‡å¥–åŠ±ä¸º 49.52
ç¬¬5278æ¬¡: ç¬¬122å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 38.00, å¹³å‡å¥–åŠ±ä¸º 49.76
ç¬¬5307æ¬¡: ç¬¬123å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 29.00, å¹³å‡å¥–åŠ±ä¸º 49.84
ç¬¬5351æ¬¡: ç¬¬124å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 44.00, å¹³å‡å¥–åŠ±ä¸º 50.12
ç»è¿‡5351è½®å®Œæˆ124å±€æ¸¸æˆ!</pre>
</div>
<p><span style="font-size: 16px;"><span style="font-family: 'comic sans ms', sans-serif;">å½“æ·»åŠ baselineæ—¶ï¼š</span></span></p>
<div class="cnblogs_Highlighter">
<pre class="brush:python;gutter:true;">ç¬¬18æ¬¡: ç¬¬1å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 18.00, å¹³å‡å¥–åŠ±ä¸º 18.00
ç¬¬36æ¬¡: ç¬¬2å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 18.00, å¹³å‡å¥–åŠ±ä¸º 18.00
ç¬¬58æ¬¡: ç¬¬3å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 22.00, å¹³å‡å¥–åŠ±ä¸º 19.33
ç¬¬80æ¬¡: ç¬¬4å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 22.00, å¹³å‡å¥–åŠ±ä¸º 20.00
ç¬¬92æ¬¡: ç¬¬5å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 12.00, å¹³å‡å¥–åŠ±ä¸º 18.40
ç¬¬116æ¬¡: ç¬¬6å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 24.00, å¹³å‡å¥–åŠ±ä¸º 19.33
ç¬¬187æ¬¡: ç¬¬7å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 71.00, å¹³å‡å¥–åŠ±ä¸º 26.71
ç¬¬228æ¬¡: ç¬¬8å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 41.00, å¹³å‡å¥–åŠ±ä¸º 28.50
ç¬¬253æ¬¡: ç¬¬9å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 25.00, å¹³å‡å¥–åŠ±ä¸º 28.11
ç¬¬276æ¬¡: ç¬¬10å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 23.00, å¹³å‡å¥–åŠ±ä¸º 27.60
ç¬¬352æ¬¡: ç¬¬11å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 76.00, å¹³å‡å¥–åŠ±ä¸º 32.00
ç¬¬459æ¬¡: ç¬¬12å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º107.00, å¹³å‡å¥–åŠ±ä¸º 38.25
ç¬¬514æ¬¡: ç¬¬13å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 55.00, å¹³å‡å¥–åŠ±ä¸º 39.54
ç¬¬557æ¬¡: ç¬¬14å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º 43.00, å¹³å‡å¥–åŠ±ä¸º 39.79
ç¬¬659æ¬¡: ç¬¬15å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º102.00, å¹³å‡å¥–åŠ±ä¸º 43.93
ç¬¬859æ¬¡: ç¬¬16å±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º200.00, å¹³å‡å¥–åŠ±ä¸º 53.69
ç»è¿‡859è½®å®Œæˆ16å±€æ¸¸æˆ!</pre>
</div>
<p><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220301135848565-1495644129.png" alt="" width="785" height="313" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">The rewards of training episodes without the baseline (black) and with the baseline (blue)</span></p>
<p><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220301140007243-41941615.png" alt="" width="791" height="314" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">The average rewards of training episodes without the baseline (black) and with the baseline (blue)</span></p>
<p><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220301140129084-1684950076.png" alt="" width="797" height="314" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Total loss&nbsp;without the baseline (black) and with the baseline (blue)</span></p>
<p><img src="https://img2022.cnblogs.com/blog/1027447/202203/1027447-20220301140151375-604342470.png" alt="" width="792" height="313" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">Episodes&nbsp;without the baseline (black) and with the baseline (blue)</span></p>
<p><span style="font-size: 16px;"><span style="font-family: 'comic sans ms', sans-serif;">æ‰“å¼€</span>tensorboardï¼š</span></p>
<div class="cnblogs_Highlighter">
<pre class="brush:python;gutter:true;">activate RL
cd ..
D:
cd D:\xxx\Deep-Reinforcement-Learning-Hands-On-Second-Edition-master\Chapter11
tensorboard --logdir=./runs
http://localhost:6006/</pre>
</div>
<h2><span style="font-family: 'comic sans ms', sans-serif;">7. å‚è€ƒæ–‡çŒ®</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[1]&nbsp;<a href="https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition" rel="noopener" target="_blank">https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[2]&nbsp;<a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/kailugaji/p/15354491.html">å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)</a>&nbsp;- å‡¯é²å˜å‰ - åšå®¢å›­</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">[3]&nbsp;<a href="https://github.com/kailugaji/Hands-on-Reinforcement-Learning/tree/main/03%20Policy%20Gradients" target="_blank">https://github.com/kailugaji/Hands-on-Reinforcement-Learning/tree/main/03%20Policy%20Gradients</a></span></p>