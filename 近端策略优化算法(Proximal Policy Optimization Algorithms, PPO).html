<h1 style="text-align: center;"><span style="font-family: 'comic sans ms', sans-serif;">近端策略优化算法(Proximal Policy Optimization Algorithms, PPO)</span></h1>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">作者：凯鲁嘎吉 - 博客园&nbsp;<a href="http://www.cnblogs.com/kailugaji/" target="_blank">http://www.cnblogs.com/kailugaji/</a></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; 这篇博文是Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. <a href="https://arxiv.org/pdf/1707.06347.pdf" target="_blank">Proximal policy optimization algorithms</a>. Advances in Neural Information Processing Systems, 2017.的阅读笔记，用来介绍PPO优化方法及其一些公式的推导。文中给出了三种优化方法，其中第三种是第一种的拓展，这两种使用广泛，第二种实验验证效果不好，但也是一个小技巧。阅读本文，需要事先了解<a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/kailugaji/p/15388913.html">信赖域策略优化(Trust Region Policy Optimization, TRPO)</a>，从Proximal这个词汇中，可以联想到<a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/kailugaji/p/14613210.html">一类涉及矩阵范数的优化问题</a>中的软阈值算子(soft thresholding/shrinkage operator)以及<a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/kailugaji/p/11688004.html">图Lasso求逆协方差矩阵(Graphical Lasso for inverse covariance matrix)</a>中使用<a href="https://www.cnblogs.com/kailugaji/p/11688004.html#_lab2_0_4" rel="nofollow">近端梯度下降(Proximal Gradient Descent, PGD)求解Lasso问题</a>。更多强化学习内容，请看：<a href="https://www.cnblogs.com/kailugaji/category/2038931.html" target="_blank">随笔分类 - Reinforcement Learning</a>。</span></p>
<h2><span style="font-family: 'comic sans ms', sans-serif;">1. 前提知识</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">策略梯度法(Policy Gradient Methods)与信赖域策略优化(Trust Region Policy Optimization, TRPO)</span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211012095011096-1340470520.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif; font-size: 16px;">&nbsp; &nbsp; 由于TRPO使用了一个硬约束来计算策略梯度，因此很难选择一个在不同问题中都表现良好的单一约束值。</span></p>
<h2 id="1634003218648"><span style="font-family: 'comic sans ms', sans-serif;">2.&nbsp;方法一：Clipped Surrogate Objective</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211012095034269-1597541215.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211012095040202-255534208.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2 id="1634003247811"><span style="font-family: 'comic sans ms', sans-serif;">3.&nbsp;方法二：Adaptive KL Penalty Coefficient</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211012095104451-2139244826.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2 id="1634003271984"><span style="font-family: 'comic sans ms', sans-serif;">4.&nbsp;方法三：Actor-Critic-Style Algorithm</span></h2>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211012095128114-1247936516.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211012095134216-1173736811.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<p><span style="font-family: 'comic sans ms', sans-serif;"><img src="https://img2020.cnblogs.com/blog/1027447/202110/1027447-20211012095139526-1293878004.png" alt="" loading="lazy" style="display: block; margin-left: auto; margin-right: auto;" /></span></p>
<h2 id="1634003307093"><span style="font-family: 'comic sans ms', sans-serif;">5. 参考文献</span></h2>
<p><span style="font-size: 16px; font-family: 'comic sans ms', sans-serif;">[1] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. <a href="https://arxiv.org/pdf/1707.06347.pdf" target="_blank">Proximal policy optimization algorithms</a>. Advances in Neural Information Processing Systems, 2017.</span><br /><span style="font-size: 16px; font-family: 'comic sans ms', sans-serif;">[2] Proximal Policy Optimization &mdash; Spinning Up documentation <a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html" target="_blank">https://spinningup.openai.com/en/latest/algorithms/ppo.html</a></span><br /><span style="font-size: 16px; font-family: 'comic sans ms', sans-serif;">[3] V. Mnih, A.Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, K. Kavukcuoglu. <a href="http://proceedings.mlr.press/v48/mniha16.html" target="_blank">Asynchronous Methods for Deep Reinforcement Learning</a>. ICML, 2016.</span><br /><span style="font-size: 16px; font-family: 'comic sans ms', sans-serif;">[4] Proximal Policy Optimization Algorithms, slides, <a href="https://dvl.in.tum.de/slides/automl-ss19/01_stadler_ppo.pdf" target="_blank">https://dvl.in.tum.de/slides/automl-ss19/01_stadler_ppo.pdf</a></span></p>